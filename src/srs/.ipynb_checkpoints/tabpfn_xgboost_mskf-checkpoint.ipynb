{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "672cd7e5-7478-4b75-a4db-810476f06233",
   "metadata": {},
   "source": [
    "# tabpfn_xgboost_mskf.ipynb\n",
    "## å®Ÿé¨“æ¡ä»¶\n",
    "* å­¦ç¿’æ™‚ã«greeks.csvã®Epsilonä½¿ç”¨, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®Epsilonã®æœ€å¤§å€¤+1ã¨ã™ã‚‹\n",
    "* æ¬ æå€¤ã¯ä¸­å¤®å€¤ã§è£œå®Œ\n",
    "* greeks.csvã®Alphaã‚’äºˆæ¸¬ã€äºˆæ¸¬å¾Œã«A->0, (B, G, D)->1ã«å¤‰æ›\n",
    "* CVã¯MultilabelStratifiedKFoldã§ã€Beta, Gamma, Deltaã®ã‚¯ãƒ©ã‚¹å‰²åˆãŒåŒã˜ã«ãªã‚‹ã‚ˆã†ã«åˆ†å‰²\n",
    "### çµæœ\n",
    "* CV: 0.19097697170787517\n",
    "## å¤‰æ›´ç‚¹(2023-08-05 20:59)\n",
    "* XGBClassifierã®sample_weightã‚’è¨­å®š\n",
    "* sample_weightã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«Alphaã‚’ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "### çµæœ\n",
    "* CV: 0.18313566432976108\n",
    "## å¤‰æ›´ç‚¹(2023-08-07 0:29)\n",
    "* XGBClassifierã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã•ã‚‰ã«ç´°ã‹ãè¨­å®š\n",
    "### çµæœ\n",
    "* CV: 0.1762083652384992"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b8938-dc60-4a04-b7d3-d3b2ec25170c",
   "metadata": {},
   "source": [
    "## TabPFNã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "### äº‹å‰ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«: \n",
    "* TabPFN: <https://www.kaggle.com/datasets/carlmcbrideellis/tabpfn-019-whl>\n",
    "* MultilabelStratifiedKFold: <https://www.kaggle.com/datasets/tilii7/iterative-stratification-017>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d61f413-8b49-4e6f-b89d-491fbbb50abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KaggleNotebookã§ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã™\n",
    "# !pip install -q /kaggle/input/tabpfn-019-whl/tabpfn-0.1.9-py3-none-any.whl\n",
    "# !mkdir /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n",
    "# !cp /kaggle/input/tabpfn-019-whl/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/\n",
    "# !pip install -q /kaggle/input/iterative-stratification-017/iterative_stratification-0.1.7-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f9a60a-4bfa-423b-b613-a372d4c63685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "# model\n",
    "from sklearn.base import BaseEstimator\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import lightgbm as lgb\n",
    "from tabpfn import TabPFNClassifier\n",
    "# over/under sampling\n",
    "from imblearn.over_sampling import SMOTE # SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# Imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif# Feature Selection\n",
    "import category_encoders as encoders\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "# cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "# others\n",
    "from datetime import date, datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# ç’°å¢ƒã‚’æŒ‡å®š\n",
    "env = 'local'\n",
    "# env = 'kaggle'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d4b8a-f3c7-4274-8f37-b40dd2fe7008",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39222989-8739-458c-99c0-c4cff87d149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æŒ‡å®š\n",
    "if env == 'local':\n",
    "    BASE_DIR = '../../data'\n",
    "elif env == 'kaggle':\n",
    "    BASE_DIR = '/kaggle/input/icr-identify-age-related-conditions/'\n",
    "else:\n",
    "    raise ValueError(\"Invalid environment. Set env as 'local' or 'kaggle'.\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "train_df = pd.read_csv(f'{BASE_DIR}/train.csv')\n",
    "# train_df = pd.read_csv(f'{BASE_DIR}/train_integerized.csv')\n",
    "greeks_df = pd.read_csv(f'{BASE_DIR}/greeks.csv')\n",
    "test_df = pd.read_csv(f'{BASE_DIR}/test.csv')\n",
    "submission_df = pd.read_csv(f'{BASE_DIR}/sample_submission.csv')\n",
    "\n",
    "# greeksã¨çµåˆ\n",
    "train_df = pd.merge(train_df, greeks_df[['Id', 'Alpha', 'Epsilon']], on='Id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e192c07-7cc0-43b3-940a-795381b7d93c",
   "metadata": {},
   "source": [
    "greeksã®Aã¯Class0ã«ã€B, G, Dã¯Class1ã«ç›¸å½“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f050385-9970-4624-8654-8ed504030602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alpha\n",
       "A    509\n",
       "B     61\n",
       "G     29\n",
       "D     18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greeks_df.Alpha.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788c1eb-f369-4df1-980a-587c443c43c1",
   "metadata": {},
   "source": [
    "## Epsilonã‚’ç‰¹å¾´é‡ã«è¿½åŠ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3e3b4b-c47c-499e-a6cf-0b1f42ad96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¬ æå€¤ä»¥å¤–ã®æ—¥ä»˜ã‚’ã‚°ãƒ¬ã‚´ãƒªã‚ªæš¦ã®åºæ•°å½¢å¼ï¼ˆ1å¹´1æœˆ1æ—¥ã‚’1ã¨ã—ã€1æ—¥ãšã¤å¢—ã‚„ã—ã¦ã„ãï¼‰ã«å¤‰æ›\n",
    "train_df.Epsilon[train_df.Epsilon != 'Unknown'] = train_df.Epsilon[train_df.Epsilon != 'Unknown']\\\n",
    "                                        .map(lambda x: datetime.strptime(x, '%m/%d/%Y').toordinal())\n",
    "# æ¬ æå€¤ã‚’np.nanã«å¤‰æ›\n",
    "train_df.Epsilon[train_df.Epsilon == 'Unknown'] = np.nan\n",
    "\n",
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°ã«åˆ†å‰²\n",
    "X_train = train_df.drop(['Id', 'EJ', 'Alpha', 'Class'], axis=1)\n",
    "y_train = train_df[['Class', 'Alpha']]\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ•°å€¤ãƒ‡ãƒ¼ã‚¿ä»¥å¤–ã‚’å‰Šé™¤\n",
    "X_test = test_df.drop(['Id', 'EJ'], axis=1)\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æœ€å¤§å€¤+1ã¨ã™ã‚‹\n",
    "X_test['Epsilon'] = train_df.Epsilon.max()+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19307475-d571-443e-9464-bc470dedb377",
   "metadata": {},
   "source": [
    "## Alphaã‚’ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45fa65ec-8711-4b29-af4f-eab77e820be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’åˆæœŸåŒ–\n",
    "le = LabelEncoder()\n",
    "# yã‚’æ•°å€¤ã«å¤‰æ›\n",
    "y_train['Alpha'] = le.fit_transform(y_train['Alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c29971-dd6e-4f2c-8b4f-808901cbea86",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«ã€è©•ä¾¡åŸºæº–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebac07ef-bc76-468e-af4c-9270e567beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šå€¤\n",
    "xgb_params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.2,\n",
    "    'max_depth': 18, \n",
    "    'subsample': 0.5236088397410353, \n",
    "    'colsample_bytree': 0.9081020201822949, \n",
    "    'min_child_weight': 1, \n",
    "    'gamma': 0.018902786999403336, \n",
    "    'lambda': 0.002946024021403057, \n",
    "    'alpha': 0.21131772406300453\n",
    "}\n",
    "\n",
    "class WeightedEns(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.models = [\n",
    "            XGBClassifier(),\n",
    "            TabPFNClassifier(N_ensemble_configurations=256,device='cuda:0')\n",
    "        ]\n",
    "        self.imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "        # self.imputer = KNNImputer(n_neighbors=50)\n",
    "    \n",
    "    def fit(self, X, y, weights=None):\n",
    "        classes, y = np.unique(y, return_inverse=True)\n",
    "        self.classes_ = classes\n",
    "        X = self.imputer.fit_transform(X)\n",
    "        for i, model in enumerate(self.models):\n",
    "            if isinstance(model, XGBClassifier):\n",
    "                model.set_params(**xgb_params)\n",
    "                model.fit(X, y, sample_weight=weights) # æ±ºå®šæœ¨ã§ã¯weightã‚’è€ƒæ…®ã™ã‚‹\n",
    "            else:\n",
    "                model.fit(X,y)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = self.imputer.transform(X)\n",
    "        ps = np.stack([model.predict_proba(X) for model in self.models])\n",
    "        p = np.mean(ps,axis=0)\n",
    "        class_0_est_instances = p[:,0].sum()\n",
    "        others_est_instances = p[:,1:].sum()\n",
    "        # we reweight the probs, since the loss is also balanced like this\n",
    "        # our models out of the box optimize CE\n",
    "        # with these changes they optimize balanced CE\n",
    "        new_p = p * np.array([[1/(class_0_est_instances if i==0 else others_est_instances) for i in range(p.shape[1])]])\n",
    "        new_p = new_p / np.sum(new_p,axis=1,keepdims=1)\n",
    "        return np.concatenate((new_p[:,:1],np.sum(new_p[:,1:],1,keepdims=True)), 1)\n",
    "\n",
    "# è©•ä¾¡åŸºæº–\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability ğ‘ is replaced with max(min(ğ‘,1âˆ’10âˆ’15),10âˆ’15)\n",
    "    y_pred = np.maximum(np.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1-y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c8c06-a9a1-47c7-b2bf-0b697cf7ccd9",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf0b57b6-587b-44d0-a6af-76e9e7828b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 2\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 3\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 4\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 5\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 6\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 7\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 8\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 9\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 10\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "our out of folds CV score is 0.1762083652384992\n"
     ]
    }
   ],
   "source": [
    "# åˆæœŸå€¤\n",
    "seed = 779292\n",
    "folds = 10\n",
    "labels = greeks_df[['Beta', 'Gamma', 'Delta']] # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆ†å‰²ã§è€ƒæ…®ã™ã‚‹ç‰¹å¾´é‡\n",
    "\n",
    "# å„åˆ†å‰²ã”ã¨ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’æ ¼ç´\n",
    "scores = 0\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "models = []\n",
    "# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆ†å‰²æ•°ã‚’æŒ‡å®š\n",
    "mskf = MultilabelStratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(mskf.split(X_train, labels)):\n",
    "    # é€²è¡ŒçŠ¶æ³\n",
    "    print('fold: {}'.format(fold+1))\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²\n",
    "    X_train_fold = X_train.iloc[train_index]\n",
    "    y_train_fold = y_train['Alpha'].iloc[train_index]\n",
    "    X_valid_fold = X_train.iloc[valid_index]\n",
    "    y_valid_fold = y_train['Class'].iloc[valid_index]\n",
    "\n",
    "    # Alphaã®ãƒ©ãƒ™ãƒ«ã«é‡ã¿ã¥ã‘ã‚’ã™ã‚‹\n",
    "    sample_weights = compute_sample_weight(class_weight='balanced', y=y_train_fold)\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã€äºˆæ¸¬ã‚’å‡ºåŠ›\n",
    "    model = WeightedEns()\n",
    "    model.fit(X_train_fold,y_train_fold, weights=sample_weights)\n",
    "    valid_preds = model.predict_proba(X_valid_fold)[:, 1]\n",
    "\n",
    "    # è©•ä¾¡\n",
    "    val_score = balanced_log_loss(y_valid_fold, valid_preds)\n",
    "    # ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜\n",
    "    scores += val_score\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "    models.append(model)\n",
    "    \n",
    "# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®å¹³å‡å€¤ã‚’è¨ˆç®—\n",
    "cv_score = scores /  folds\n",
    "print(f'our out of folds CV score is {cv_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c79577-272f-431c-8757-165002c1010d",
   "metadata": {},
   "source": [
    "## æå‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b06790e-ee1e-4c24-9aad-d48062526734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå‡ºç”¨ã«å€¤ã‚’å¤‰æ›\n",
    "if env == 'kaggle':\n",
    "    # äºˆæ¸¬\n",
    "    # å„åˆ†å‰²ã”ã¨ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äºˆæ¸¬å€¤ã‚’æ ¼ç´\n",
    "    preds = np.zeros(len(X_test))\n",
    "    for i in range(len(models)):\n",
    "        # pred = models[i].predict(xgb.DMatrix(test_df.drop(['Id', 'EJ'], axis=1)), iteration_range=(0, models[i].best_iteration))\n",
    "        pred = models[i].predict(X_test)\n",
    "        preds += pred\n",
    "    test_pred = preds / folds\n",
    "\n",
    "    # æå‡º\n",
    "    submission = pd.DataFrame(columns = submission_df.columns)\n",
    "    submission['Id'] = test_df['Id']\n",
    "    submission['class_0'] = 1 - test_pred\n",
    "    submission['class_1'] = test_pred\n",
    "    submission.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
