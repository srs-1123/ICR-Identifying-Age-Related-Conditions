{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52304d70",
   "metadata": {},
   "source": [
    "# xgb+lgb+tabpfn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443f30b",
   "metadata": {},
   "source": [
    "## å®Ÿé¨“çµæœ\n",
    "* xgb(ç‰¹å¾´é‡æŠ½å‡º)  \n",
    "CV: 0.3058027696858661\n",
    "* lgb(ç‰¹å¾´é‡æŠ½å‡º)  \n",
    "CV: 0.31445132158950145\n",
    "* TabPFN(ç‰¹å¾´é‡æŠ½å‡º)  \n",
    "CV: 0.430984415297508"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c2bf9-2843-447c-bfea-2140254115f3",
   "metadata": {},
   "source": [
    "## TabPFNã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "* äº‹å‰ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«: <https://www.kaggle.com/datasets/carlmcbrideellis/tabpfn-019-whl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d6f7c16-2dc2-40db-8dd9-c565c4348d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q /kaggle/input/tabpfn-019-whl/tabpfn-0.1.9-py3-none-any.whl\n",
    "# !mkdir /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n",
    "# !cp /kaggle/input/tabpfn-019-whl/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c565dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from tabpfn import TabPFNClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE # SMOTE\n",
    "from sklearn.impute import KNNImputer # kNN Imputation\n",
    "from sklearn.feature_selection import SelectKBest, f_classif# Feature Selection\n",
    "# Data Encoder and Scaler\n",
    "import category_encoders as encoders\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# ç’°å¢ƒã‚’æŒ‡å®š\n",
    "env = 'local'\n",
    "# env = 'kaggle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b138da54-574e-492d-8e40-e463f9c6be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_imputer(train_df, test_df):\n",
    "    # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "    numerical_columns = train_df.drop(['Id', 'EJ', 'Class'], axis=1).columns\n",
    "\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ä»£å…¥\n",
    "    train_df_imputed = pd.DataFrame(imputer.fit_transform(train_df[numerical_columns]), columns=numerical_columns)\n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ä»£å…¥\n",
    "    test_df_imputed = pd.DataFrame(imputer.transform(test_df[numerical_columns]), columns=numerical_columns)\n",
    "\n",
    "    # å…ƒã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚‚æ¬ æå€¤ã‚’è£œå®Œã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹\n",
    "    train_df = train_df.drop(numerical_columns, axis=1)\n",
    "    train_df = pd.concat([train_df, train_df_imputed], axis=1)\n",
    "\n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ¬ æå€¤ã‚’ä»£å…¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹\n",
    "    test_df = test_df.drop(numerical_columns, axis=1)\n",
    "    test_df = pd.concat([test_df, test_df_imputed], axis=1)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "def select_k_best(train_df, test_df, pvalue_upper_limit = 0.1, fscore_lower_limit = 5):\n",
    "    # æ¬ æå€¤ã®è£œå®Œ\n",
    "    train_df, test_df = knn_imputer(train_df, test_df)\n",
    "    # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®åˆ—\n",
    "    numerical_columns = train_df.drop(['Id', 'EJ', 'Class'], axis=1).columns\n",
    "    features = pd.DataFrame(index=numerical_columns, columns=[\"F_value\", \"p_value\"])\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°ã«åˆ†å‰²\n",
    "    X_train = train_df.drop(['Id', 'EJ', 'Class'], axis=1)\n",
    "    y_train = train_df['Class']\n",
    "    # y_train.columns = ['Class']\n",
    "    '''Få€¤ã¨på€¤ã‚’è¨ˆç®—'''\n",
    "    # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "    #     å›å¸°: f_regression, mutual_info_regression\n",
    "    #     åˆ†é¡: chi2, f_classif(åˆ†æ•£åˆ†æã®Få€¤), mutual_info_classif\n",
    "    # ã“ã®æ™‚ç‚¹ã§ã¯kã‚’ã‚‚ã¨ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜ã«ã™ã‚‹\n",
    "    fs = SelectKBest(score_func=f_classif, k=len(X_train.columns))\n",
    "    # ç‰¹å¾´é‡é¸æŠ\n",
    "    X_selected = fs.fit_transform(X_train, y_train.values)\n",
    "\n",
    "    '''é¸æŠã—ãŸFå€¤ã¨på€¤ã¨è¨­å®šã—ãŸé–¾å€¤ã‚’ç”¨ã„ã¦ç‰¹å¾´é‡ã‚’é¸æŠ'''\n",
    "    new_features = [] # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã‚’æ ¼ç´\n",
    "    drop_features = [] # ä½¿ã‚ãªã„ç‰¹å¾´é‡ã‚’æ ¼ç´\n",
    "\n",
    "    # Få€¤ãŒå¤§ããã€på€¤ã®å°ã•ã„ç‰¹å¾´é‡ã‚’é¸æŠ\n",
    "    for i in range(len(X_train.columns)):\n",
    "        # Få€¤ã¨på€¤ã‚’æ ¼ç´\n",
    "        features.loc[X_train.columns[i], \"F_value\"] = fs.scores_[i]\n",
    "        features.loc[X_train.columns[i], \"p_value\"] = fs.pvalues_[i]\n",
    "        \n",
    "        if fs.pvalues_[i] <= pvalue_upper_limit and fs.scores_[i] >= fscore_lower_limit:\n",
    "            new_features.append(X_train.columns[i])\n",
    "        else:\n",
    "            drop_features.append(X_train.columns[i])\n",
    "    \n",
    "    features = features.loc[new_features, :] # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã ã‘ã‚’featuresã«ä¿å­˜\n",
    "    features = features.sort_values(\"F_value\", ascending=False)# Få€¤ãŒå¤§ãã„é †ã«ã‚½ãƒ¼ãƒˆ\n",
    "    \n",
    "    return features, drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c555c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    '''å‰å‡¦ç†ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹'''\n",
    "    def __init__(self, train_df, test_df):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.numerical_columns = train_df.drop(['Id', 'EJ', 'Class'], axis=1).columns\n",
    "        self.features = pd.DataFrame(index=self.numerical_columns, columns=[\"F_value\", \"p_value\"])\n",
    "        \n",
    "    def knn_imputer(self):\n",
    "        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        \n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "        \n",
    "        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ä»£å…¥\n",
    "        train_df_imputed = pd.DataFrame(imputer.fit_transform(temp_train_df[self.numerical_columns]), columns=self.numerical_columns)\n",
    "        \n",
    "        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ä»£å…¥\n",
    "        test_df_imputed = pd.DataFrame(imputer.transform(temp_test_df[self.numerical_columns]), columns=self.numerical_columns)\n",
    "\n",
    "        # å…ƒã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚‚æ¬ æå€¤ã‚’è£œå®Œã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹\n",
    "        temp_train_df = temp_train_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_train_df = pd.concat([temp_train_df, train_df_imputed], axis=1)\n",
    "\n",
    "        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ¬ æå€¤ã‚’ä»£å…¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹\n",
    "        temp_test_df = temp_test_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_test_df = pd.concat([temp_test_df, test_df_imputed], axis=1)\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "    \n",
    "    def clip_outliers_ver1(self):\n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "\n",
    "        first_quartiles = temp_train_df[self.numerical_columns].quantile(0.25) # ç¬¬ï¼‘å››åˆ†ä½æ•°\n",
    "        third_quartiles = temp_train_df[self.numerical_columns].quantile(0.75) # ç¬¬ï¼“å››åˆ†ä½æ•°\n",
    "        iqr = third_quartiles - first_quartiles # å››åˆ†ä½ç¯„å›²\n",
    "\n",
    "        lower_bound = first_quartiles - (iqr * 1.5) #å¤–ã‚Œå€¤ã®ä¸‹é™\n",
    "        upper_bound = third_quartiles + (iqr * 1.5) #å¤–ã‚Œå€¤ã®ä¸Šé™\n",
    "\n",
    "        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä¸¡æ–¹ã«å¯¾ã—ã¦å‡¦ç†ã‚’è¡Œã†\n",
    "        for df in [temp_train_df, temp_test_df]:\n",
    "            df[self.numerical_columns] = df[self.numerical_columns].clip(lower_bound, upper_bound, axis=1)\n",
    "\n",
    "        return temp_train_df, temp_test_df\n",
    "    \n",
    "    def clip_outliers(self):\n",
    "        '''è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§åˆ¥ã€…ã«ä¸Šé™ã€ä¸‹é™ã‚’è¨ˆç®—ã™ã‚‹ã‚ˆã†å¤‰æ›´'''\n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´\n",
    "        clipped_df = []\n",
    "        \n",
    "        for df in [self.train_df, self.test_df]:\n",
    "            temp_df = df\n",
    "            first_quartiles = temp_df[self.numerical_columns].quantile(0.25) # ç¬¬ï¼‘å››åˆ†ä½æ•°\n",
    "            third_quartiles = temp_df[self.numerical_columns].quantile(0.75) # ç¬¬ï¼“å››åˆ†ä½æ•°\n",
    "            iqr = third_quartiles - first_quartiles # å››åˆ†ä½ç¯„å›²\n",
    "\n",
    "            lower_bound = first_quartiles - (iqr * 1.5) #å¤–ã‚Œå€¤ã®ä¸‹é™\n",
    "            upper_bound = third_quartiles + (iqr * 1.5) #å¤–ã‚Œå€¤ã®ä¸Šé™\n",
    "\n",
    "            temp_df[self.numerical_columns] = temp_df[self.numerical_columns].clip(lower_bound, upper_bound, axis=1)\n",
    "            clipped_df.append(temp_df)\n",
    "\n",
    "        return clipped_df[0], clipped_df[1]\n",
    "        \n",
    "    def robust_scaler(self):\n",
    "        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "\n",
    "        '''è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°'''\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŠ½å‡º\n",
    "        index = temp_train_df.index\n",
    "        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        scaler_train = scaler.fit_transform(temp_train_df[self.numerical_columns])\n",
    "        scaled_train_df = pd.DataFrame(scaler_train, columns=self.numerical_columns)\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒ¯ã‚ŠãªãŠã™\n",
    "        scaled_train_df.index = index\n",
    "\n",
    "        '''ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°'''\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŠ½å‡º\n",
    "        index = temp_test_df.index\n",
    "        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        scaler_test = scaler.fit_transform(temp_test_df[self.numerical_columns])\n",
    "        scaled_test_df = pd.DataFrame(scaler_test, columns=self.numerical_columns)\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒ¯ã‚ŠãªãŠã™\n",
    "        scaled_test_df.index = index\n",
    "        \n",
    "        # å…ƒã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚‚æ¬ æå€¤ã‚’è£œå®Œã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹\n",
    "        temp_train_df = temp_train_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_train_df = pd.concat([temp_train_df, scaled_train_df], axis=1)\n",
    "\n",
    "        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ¬ æå€¤ã‚’ä»£å…¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹\n",
    "        temp_test_df = temp_test_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_test_df = pd.concat([temp_test_df, scaled_test_df], axis=1)\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "        \n",
    "def preprocessing_pipeline(train_df, test_df):\n",
    "    # ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆ\n",
    "    preprocessor = Preprocessing(train_df, test_df)\n",
    "    \n",
    "    # å„ãƒ¡ã‚½ãƒƒãƒ‰ã‚’é †ã«å®Ÿè¡Œ\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.knn_imputer() # æ¬ æå€¤ä»£å…¥\n",
    "    # preprocessor.train_df, preprocessor.test_df = preprocessor.clip_outliers() # å¤–ã‚Œå€¤é™¤å»\n",
    "    # preprocessor.train_df, preprocessor.test_df = preprocessor.robust_scaler() # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "    \n",
    "    # print('selected features: \\n{}'.format(preprocessor.features))\n",
    "\n",
    "    # æœ€çµ‚çš„ã«å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™\n",
    "    return preprocessor.train_df, preprocessor.test_df\n",
    "\n",
    "# è©•ä¾¡åŸºæº–\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability ğ‘ is replaced with max(min(ğ‘,1âˆ’10âˆ’15),10âˆ’15)\n",
    "    y_pred = np.maximum(np.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1-y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "def balanced_log_loss_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    loss = balanced_log_loss(y_true, y_pred)\n",
    "    return 'balanced_log_loss', loss\n",
    "\n",
    "# Classã®ï¼ï¼Œï¼‘ã®å‰²åˆã‚’ãã‚Œãã‚Œè¨ˆç®—\n",
    "def calc_log_loss_weight(y_true):\n",
    "    nc = np.bincount(y_true)\n",
    "    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n",
    "    return w0, w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba2e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_training(X_train, y_train, X_valid, y_valid):\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ã‚’è¨ˆç®—\n",
    "    train_w0, train_w1 = calc_log_loss_weight(y_train)\n",
    "    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ã‚’è¨ˆç®—\n",
    "    valid_w0, valid_w1 = calc_log_loss_weight(y_valid)\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’xgbç”¨ã«å¤‰æ›\n",
    "    xgb_train = xgb.DMatrix(data=X_train, label=y_train, weight=y_train.map({0: train_w0, 1: train_w1}))\n",
    "    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’xgbç”¨ã«å¤‰æ›\n",
    "    xgb_valid = xgb.DMatrix(data=X_valid, label=y_valid, weight=y_valid.map({0: valid_w0, 1: valid_w1}))\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "    model = xgb.train(\n",
    "        CFG.xgb_params, \n",
    "        dtrain = xgb_train, \n",
    "        num_boost_round = CFG.num_boost_round,\n",
    "        evals = [(xgb_train, 'train'), (xgb_valid, 'eval')], \n",
    "        early_stopping_rounds = CFG.early_stopping_rounds,\n",
    "        verbose_eval = False, # æ•´æ•°ã«è¨­å®šã™ã‚‹ã¨ã€nå›ã”ã¨ã®ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹ãƒ†ãƒ¼ã‚¸ã§è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º\n",
    "    )\n",
    "    # æ¤œè¨¼\n",
    "    valid_preds = model.predict(xgb.DMatrix(X_valid), iteration_range=(0, model.best_ntree_limit))\n",
    "    \n",
    "    return model, valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95208b27-753f-4a80-872c-13e0c963350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_training(X_train, y_train, X_valid, y_valid):\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ã‚’è¨ˆç®—\n",
    "    train_w0, train_w1 = calc_log_loss_weight(y_train)\n",
    "    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ã‚’è¨ˆç®—\n",
    "    valid_w0, valid_w1 = calc_log_loss_weight(y_valid)\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’lgbç”¨ã«å¤‰æ›\n",
    "    lgb_train = lgb.Dataset(X_train, y_train, weight=y_train.map({0: train_w0, 1: train_w1}))\n",
    "    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’lgbç”¨ã«å¤‰æ›\n",
    "    lgb_valid = lgb.Dataset(X_valid, y_valid, weight=y_valid.map({0: valid_w0, 1: valid_w1}))\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params = CFG.lgb_params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = CFG.num_boost_round,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = CFG.early_stopping_rounds,\n",
    "        verbose_eval = False,\n",
    "    )\n",
    "    # æ¤œè¨¼\n",
    "    valid_preds = model.predict(X_valid)\n",
    "    \n",
    "    return model, valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49b5c703-444c-4c8c-944a-58b7d44e5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabpfn_training(X_train, y_train, X_valid, y_valid):\n",
    "    # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "    model = TabPFNClassifier(device='cpu', N_ensemble_configurations=256)\n",
    "    # å­¦ç¿’\n",
    "    model.fit(X_train, y_train)\n",
    "    # æ¤œè¨¼\n",
    "    valid_preds = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    return model, valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb0015ff-0100-40af-8dc6-ccf78cd18c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle('params/lgb_best_param.pkl').best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38362eb8-3c17-4953-add7-f8d40914cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    '''è¨­å®šå€¤ã‚’æ ¼ç´'''\n",
    "    num_boost_round = 926\n",
    "    early_stopping_rounds = 98\n",
    "    n_folds = 5 # å…¬å·®æ¤œè¨¼ã®åˆ†å‰²æ•°\n",
    "    n_trials = 100 # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®è©¦è¡Œå›æ•°\n",
    "    seed = 1234\n",
    "    learning_rate = 0.01\n",
    "    boosting_type = \"dart\"\n",
    "    # xgboostè¨­å®šå€¤\n",
    "    xgb_params = {\n",
    "        'objective': 'binary:logistic',# å­¦ç¿’ã‚¿ã‚¹ã‚¯\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'random_state': seed,\n",
    "        'learning_rate': learning_rate,\n",
    "        'eval_metric': 'rmse',\n",
    "        # æ¢ç´¢ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        'max_depth': 43,\n",
    "        'colsample_bytree': 0.9270015786178574,\n",
    "        'subsample': 1.0,\n",
    "        'gamma': 0.9008025641267255,\n",
    "        'lambda': 0.3150372040663734,\n",
    "        'min_child_weight': 7,\n",
    "    }\n",
    "    # light-gbmè¨­å®šå€¤\n",
    "    lgb_params = {\n",
    "        # æ¢ç´¢ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        'verbosity': -1, # å­¦ç¿’é€”ä¸­ã®æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"lambda_l1\": 1.0760072734927809e-05,\n",
    "        \"lambda_l2\": 0.17928637029753666,\n",
    "        \"num_leaves\": 152,\n",
    "        \"feature_fraction\": 0.6553649132473736,\n",
    "        \"bagging_fraction\": 0.20105587614468057,\n",
    "        \"min_child_samples\": 25,\n",
    "        \n",
    "        # å›ºå®šå€¤\n",
    "        \"boosting_type\": boosting_type,\n",
    "        \"objective\": \"binary\",\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        'seed': seed,\n",
    "        # 'n_jobs': -1, # -1ã§ã‚³ã‚¢æ•°ã‚’ãƒãƒƒã‚¯ã‚¹ã§ä½¿ã†\n",
    "        'is_unbalance':True, # ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã«Trueã«ã™ã‚‹\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "892075e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "if env == 'local':\n",
    "    BASE_DIR = '../../data'\n",
    "elif env == 'kaggle':\n",
    "    BASE_DIR = '/kaggle/input/icr-identify-age-related-conditions/'\n",
    "else:\n",
    "    raise ValueError(\"Invalid environment. Set env as 'local' or 'kaggle'.\")\n",
    "\n",
    "train_df = pd.read_csv(f'{BASE_DIR}/train.csv')\n",
    "# train_df = pd.read_csv(f'{BASE_DIR}/train_integerized.csv')\n",
    "greeks_df = pd.read_csv(f'{BASE_DIR}/greeks.csv')\n",
    "test_df = pd.read_csv(f'{BASE_DIR}/test.csv')\n",
    "submission_df = pd.read_csv(f'{BASE_DIR}/sample_submission.csv')\n",
    "\n",
    "# å‰å‡¦ç†\n",
    "# features, drop_features = select_k_best(train_df, test_df, pvalue_upper_limit = 0.1, fscore_lower_limit = 5)\n",
    "features, drop_features = select_k_best(train_df, test_df, pvalue_upper_limit = 0.01, fscore_lower_limit = 3)\n",
    "train_df = train_df.drop(drop_features, axis=1)\n",
    "test_df = test_df.drop(drop_features, axis=1)\n",
    "\n",
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°ã«åˆ†å‰²\n",
    "X_train = train_df.drop(['Id', 'EJ', 'Class'], axis=1)\n",
    "y_train = train_df['Class']\n",
    "y_train.columns = ['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6662a8aa-00fe-46f3-a266-1f2e738a94e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n",
      "xgb our out of folds CV score is 0.3058027696858661\n"
     ]
    }
   ],
   "source": [
    "# å„åˆ†å‰²ã”ã¨ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’æ ¼ç´\n",
    "scores = 0\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "models = []\n",
    "\n",
    "# K-åˆ†å‰²äº¤å·®æ¤œè¨¼(å±¤åŒ–æŠ½å‡ºæ³•)\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # é€²è¡ŒçŠ¶æ³\n",
    "    print('fold: {}'.format(fold+1))\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²\n",
    "    X_train_fold = X_train.iloc[train_index]\n",
    "    y_train_fold = y_train.iloc[train_index]\n",
    "    X_valid_fold = X_train.iloc[valid_index]\n",
    "    y_valid_fold = y_train.iloc[valid_index]\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã€äºˆæ¸¬ã‚’å‡ºåŠ›\n",
    "    model, valid_preds = xgb_training(X_train_fold, y_train_fold, X_valid_fold, y_valid_fold)\n",
    "\n",
    "    # è©•ä¾¡\n",
    "    val_score = balanced_log_loss(y_valid_fold, valid_preds)\n",
    "    # ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜\n",
    "    scores += val_score\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "    models.append(model)\n",
    "    \n",
    "# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®å¹³å‡å€¤ã‚’è¨ˆç®—\n",
    "cv_score = scores /  CFG.n_folds\n",
    "print(f'xgb our out of folds CV score is {cv_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47f5345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n",
      "lgb our out of folds CV score is 0.31445132158950145\n"
     ]
    }
   ],
   "source": [
    "# å„åˆ†å‰²ã”ã¨ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’æ ¼ç´\n",
    "scores = 0\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "models = []\n",
    "\n",
    "# K-åˆ†å‰²äº¤å·®æ¤œè¨¼(å±¤åŒ–æŠ½å‡ºæ³•)\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # é€²è¡ŒçŠ¶æ³\n",
    "    print('fold: {}'.format(fold+1))\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²\n",
    "    X_train_fold = X_train.iloc[train_index]\n",
    "    y_train_fold = y_train.iloc[train_index]\n",
    "    X_valid_fold = X_train.iloc[valid_index]\n",
    "    y_valid_fold = y_train.iloc[valid_index]\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã€äºˆæ¸¬ã‚’å‡ºåŠ›\n",
    "    model, valid_preds = lgb_training(X_train_fold, y_train_fold, X_valid_fold, y_valid_fold)\n",
    "\n",
    "    # è©•ä¾¡\n",
    "    val_score = balanced_log_loss(y_valid_fold, valid_preds)\n",
    "    # ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜\n",
    "    scores += val_score\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "    models.append(model)\n",
    "    \n",
    "# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®å¹³å‡å€¤ã‚’è¨ˆç®—\n",
    "cv_score = scores /  CFG.n_folds\n",
    "print(f'lgb our out of folds CV score is {cv_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db1fbadb-e52c-45a9-ab76-c8ed47284c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 2\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 3\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 4\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 5\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "TabPFN our out of folds CV score is 0.430984415297508\n"
     ]
    }
   ],
   "source": [
    "# å„åˆ†å‰²ã”ã¨ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’æ ¼ç´\n",
    "scores = 0\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "models = []\n",
    "\n",
    "# K-åˆ†å‰²äº¤å·®æ¤œè¨¼(å±¤åŒ–æŠ½å‡ºæ³•)\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # é€²è¡ŒçŠ¶æ³\n",
    "    print('fold: {}'.format(fold+1))\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²\n",
    "    X_train_fold = X_train.iloc[train_index]\n",
    "    y_train_fold = y_train.iloc[train_index]\n",
    "    X_valid_fold = X_train.iloc[valid_index]\n",
    "    y_valid_fold = y_train.iloc[valid_index]\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã€äºˆæ¸¬ã‚’å‡ºåŠ›\n",
    "    model, valid_preds = tabpfn_training(X_train_fold, y_train_fold, X_valid_fold, y_valid_fold)\n",
    "\n",
    "    # è©•ä¾¡\n",
    "    val_score = balanced_log_loss(y_valid_fold, valid_preds)\n",
    "    # ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜\n",
    "    scores += val_score\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "    models.append(model)\n",
    "    \n",
    "# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®å¹³å‡å€¤ã‚’è¨ˆç®—\n",
    "cv_score = scores /  CFG.n_folds\n",
    "print(f'TabPFN our out of folds CV score is {cv_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a054954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå‡ºç”¨ã«å€¤ã‚’å¤‰æ›\n",
    "if env == 'kaggle':\n",
    "    # äºˆæ¸¬\n",
    "    # å„åˆ†å‰²ã”ã¨ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äºˆæ¸¬å€¤ã‚’æ ¼ç´\n",
    "    preds = np.zeros(len(test_df.drop([\"Id\", 'EJ'], axis=1)))\n",
    "    for i in range(len(models)):\n",
    "        # pred = models[i].predict(xgb.DMatrix(test_df.drop(['Id', 'EJ'], axis=1)), iteration_range=(0, models[i].best_iteration))\n",
    "        pred = models[i].predict(test_df.drop(['Id', 'EJ'], axis=1))\n",
    "        preds += pred\n",
    "    test_pred = preds / CFG.n_folds\n",
    "\n",
    "    # æå‡º\n",
    "    submission = pd.DataFrame(columns = submission_df.columns)\n",
    "    submission['Id'] = test_df['Id']\n",
    "    submission['class_0'] = 1 - test_pred\n",
    "    submission['class_1'] = test_pred\n",
    "    submission.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
