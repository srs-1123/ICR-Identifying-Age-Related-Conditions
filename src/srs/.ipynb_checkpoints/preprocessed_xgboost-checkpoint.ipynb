{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2ebfbb",
   "metadata": {},
   "source": [
    "# preprocessed_xgboost\n",
    "* ä½•ã‚‚ãªã—  \n",
    "CV: 0.28495961062567476\n",
    "* æ¬ æå€¤è£œå®Œ  \n",
    "CV: 0.2937347187995721\n",
    "* DV, ARå‰Šé™¤  \n",
    "CV: 0.29048520579559434\n",
    "* Robust Scaler  \n",
    "CV: 0.30632285148599936\n",
    "* æ¬ æå€¤è£œå®Œ+ç‰¹å¾´é‡æŠ½å‡º+Robust Scaler  \n",
    "CV: 0.30632285148599936  \n",
    "LB: 0.24\n",
    "* æ¬ æå€¤è£œå®Œ+ç‰¹å¾´é‡æŠ½å‡º+Robust Scaler+å¤–ã‚Œå€¤é™¤å»  \n",
    "CV: 0.2904466303611915  \n",
    "LB: 0.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c565dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE # SMOTE\n",
    "from sklearn.impute import KNNImputer # kNN Imputation\n",
    "from sklearn.feature_selection import SelectKBest, f_classif# Feature Selection\n",
    "# Data Encoder and Scaler\n",
    "import category_encoders as encoders\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c555c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    '''è¨­å®šå€¤ã‚’æ ¼ç´'''\n",
    "    num_boost_round = 926\n",
    "    early_stopping_rounds = 98\n",
    "    n_folds = 5 # å…¬å·®æ¤œè¨¼ã®åˆ†å‰²æ•°\n",
    "    n_trials = 100 # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®è©¦è¡Œå›æ•°\n",
    "    seed = 1234\n",
    "    # xgboostè¨­å®šå€¤\n",
    "    xgb_params = {\n",
    "        'objective': 'binary:logistic',# å­¦ç¿’ã‚¿ã‚¹ã‚¯\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'eval_metric': 'rmse',\n",
    "        'random_state': seed,\n",
    "        'learning_rate': 0.01,\n",
    "        # æ¢ç´¢ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        'max_depth': 20,\n",
    "        'colsample_bytree': 0.6597946356014663,\n",
    "        'subsample': 0.7,\n",
    "        'gamma': 1.884172979513442,\n",
    "        'lambda': 0.006050024963423825,\n",
    "        'min_child_weight': 9,\n",
    "    }\n",
    "    \n",
    "class Preprocessing:\n",
    "    '''å‰å‡¦ç†ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹'''\n",
    "    def __init__(self, train_df, test_df):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.numerical_columns = train_df.drop(['Id', 'EJ', 'Class'], axis=1).columns\n",
    "        self.features = pd.DataFrame(index=self.numerical_columns, columns=[\"F_value\", \"p_value\"])\n",
    "        \n",
    "    def knn_imputer(self):\n",
    "        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        \n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "        \n",
    "        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ä»£å…¥\n",
    "        train_df_imputed = pd.DataFrame(imputer.fit_transform(temp_train_df[self.numerical_columns]), columns=self.numerical_columns)\n",
    "        \n",
    "        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ä»£å…¥\n",
    "        test_df_imputed = pd.DataFrame(imputer.transform(temp_test_df[self.numerical_columns]), columns=self.numerical_columns)\n",
    "\n",
    "        # å…ƒã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚‚æ¬ æå€¤ã‚’è£œå®Œã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹\n",
    "        temp_train_df = temp_train_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_train_df = pd.concat([temp_train_df, train_df_imputed], axis=1)\n",
    "\n",
    "        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ¬ æå€¤ã‚’ä»£å…¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹\n",
    "        temp_test_df = temp_test_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_test_df = pd.concat([temp_test_df, test_df_imputed], axis=1)\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "    \n",
    "    def clip_outliers(self):\n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "\n",
    "        first_quartiles = temp_train_df[self.numerical_columns].quantile(0.25) # ç¬¬ï¼‘å››åˆ†ä½æ•°\n",
    "        third_quartiles = temp_train_df[self.numerical_columns].quantile(0.75) # ç¬¬ï¼“å››åˆ†ä½æ•°\n",
    "        iqr = third_quartiles - first_quartiles # å››åˆ†ä½ç¯„å›²\n",
    "\n",
    "        lower_bound = first_quartiles - (iqr * 1.5) #å¤–ã‚Œå€¤ã®ä¸‹é™\n",
    "        upper_bound = third_quartiles + (iqr * 1.5) #å¤–ã‚Œå€¤ã®ä¸Šé™\n",
    "\n",
    "        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä¸¡æ–¹ã«å¯¾ã—ã¦å‡¦ç†ã‚’è¡Œã†\n",
    "        for df in [temp_train_df, temp_test_df]:\n",
    "            df[self.numerical_columns] = df[self.numerical_columns].clip(lower_bound, upper_bound, axis=1)\n",
    "\n",
    "        return temp_train_df, temp_test_df\n",
    "        \n",
    "    def robust_scaler(self):\n",
    "        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "\n",
    "        '''è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°'''\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŠ½å‡º\n",
    "        index = temp_train_df.index\n",
    "        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        scaler_train = scaler.fit_transform(temp_train_df[self.numerical_columns])\n",
    "        scaled_train_df = pd.DataFrame(scaler_train, columns=self.numerical_columns)\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒ¯ã‚ŠãªãŠã™\n",
    "        scaled_train_df.index = index\n",
    "\n",
    "        '''ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°'''\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŠ½å‡º\n",
    "        index = temp_test_df.index\n",
    "        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        scaler_test = scaler.fit_transform(temp_test_df[self.numerical_columns])\n",
    "        scaled_test_df = pd.DataFrame(scaler_test, columns=self.numerical_columns)\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒ¯ã‚ŠãªãŠã™\n",
    "        scaled_test_df.index = index\n",
    "        \n",
    "        # å…ƒã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚‚æ¬ æå€¤ã‚’è£œå®Œã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹\n",
    "        temp_train_df = temp_train_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_train_df = pd.concat([temp_train_df, scaled_train_df], axis=1)\n",
    "\n",
    "        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ¬ æå€¤ã‚’ä»£å…¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹\n",
    "        temp_test_df = temp_test_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_test_df = pd.concat([temp_test_df, scaled_test_df], axis=1)\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "    \n",
    "    def select_k_best(self, pvalue_upper_limit = 0.1, fscore_lower_limit = 5):\n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "        \n",
    "        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°ã«åˆ†å‰²\n",
    "        X_train = temp_train_df.drop(['Id', 'EJ', 'Class'], axis=1)\n",
    "        y_train = temp_train_df['Class']\n",
    "        # y_train.columns = ['Class']\n",
    "        '''Få€¤ã¨på€¤ã‚’è¨ˆç®—'''\n",
    "        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "        #     å›å¸°: f_regression, mutual_info_regression\n",
    "        #     åˆ†é¡: chi2, f_classif(åˆ†æ•£åˆ†æã®Få€¤), mutual_info_classif\n",
    "        # ã“ã®æ™‚ç‚¹ã§ã¯kã‚’ã‚‚ã¨ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜ã«ã™ã‚‹\n",
    "        fs = SelectKBest(score_func=f_classif, k=len(X_train.columns))\n",
    "        # ç‰¹å¾´é‡é¸æŠ\n",
    "        X_selected = fs.fit_transform(X_train, y_train.values)\n",
    "\n",
    "        '''é¸æŠã—ãŸFå€¤ã¨på€¤ã¨è¨­å®šã—ãŸé–¾å€¤ã‚’ç”¨ã„ã¦ç‰¹å¾´é‡ã‚’é¸æŠ'''\n",
    "        new_features = [] # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã‚’æ ¼ç´\n",
    "        drop_features = [] # ä½¿ã‚ãªã„ç‰¹å¾´é‡ã‚’æ ¼ç´\n",
    "\n",
    "        # Få€¤ãŒå¤§ããã€på€¤ã®å°ã•ã„ç‰¹å¾´é‡ã‚’é¸æŠ\n",
    "        for i in range(len(X_train.columns)):\n",
    "            # Få€¤ã¨på€¤ã‚’æ ¼ç´\n",
    "            self.features.loc[X_train.columns[i], \"F_value\"] = fs.scores_[i]\n",
    "            self.features.loc[X_train.columns[i], \"p_value\"] = fs.pvalues_[i]\n",
    "            \n",
    "            if fs.pvalues_[i] <= pvalue_upper_limit and fs.scores_[i] >= fscore_lower_limit:\n",
    "                new_features.append(X_train.columns[i])\n",
    "            else:\n",
    "                drop_features.append(X_train.columns[i])\n",
    "\n",
    "        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é¸æŠã—ãŸç‰¹å¾´é‡ã‚’æŠ½å‡º        \n",
    "        X_selected_final = pd.DataFrame(X_selected)\n",
    "        X_selected_final.columns = X_train.columns\n",
    "        X_selected_final = X_selected_final[new_features]\n",
    "        # print('=' * 30)\n",
    "        # print('After the SelectKBest = {}'.format(X_selected_final.shape))\n",
    "        # print('Drop-out Features = {}'.format(len(drop_features)))\n",
    "\n",
    "        # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã«åæ˜ \n",
    "        # X_train = X_train.drop(drop_features, axis=1)\n",
    "        temp_train_df = temp_train_df.drop(drop_features, axis=1)\n",
    "        temp_test_df = temp_test_df.drop(drop_features, axis=1)\n",
    "        \n",
    "        self.features = self.features.loc[new_features, :] # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã ã‘ã‚’featuresã«ä¿å­˜\n",
    "        self.features = self.features.sort_values(\"F_value\", ascending=False)# Få€¤ãŒå¤§ãã„é †ã«ã‚½ãƒ¼ãƒˆ\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "        \n",
    "def preprocessing_pipeline(train_df, test_df):\n",
    "    # ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆ\n",
    "    preprocessor = Preprocessing(train_df, test_df)\n",
    "    \n",
    "    # å„ãƒ¡ã‚½ãƒƒãƒ‰ã‚’é †ã«å®Ÿè¡Œ\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.knn_imputer() # æ¬ æå€¤ä»£å…¥\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.clip_outliers() # å¤–ã‚Œå€¤é™¤å»\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.robust_scaler() # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.select_k_best(pvalue_upper_limit = 0.1, fscore_lower_limit = 5) # ç‰¹å¾´é‡é¸æŠ\n",
    "    \n",
    "    print('selected features: \\n{}'.format(preprocessor.features))\n",
    "\n",
    "    # æœ€çµ‚çš„ã«å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™\n",
    "    return preprocessor.train_df, preprocessor.test_df\n",
    "\n",
    "# è©•ä¾¡åŸºæº–\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability ğ‘ is replaced with max(min(ğ‘,1âˆ’10âˆ’15),10âˆ’15)\n",
    "    y_pred = np.maximum(np.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1-y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "# Classã®ï¼ï¼Œï¼‘ã®å‰²åˆã‚’ãã‚Œãã‚Œè¨ˆç®—\n",
    "def calc_log_loss_weight(y_true):\n",
    "    nc = np.bincount(y_true)\n",
    "    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n",
    "    return w0, w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b8ce8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected features: \n",
      "        F_value   p_value\n",
      "DU   130.715488       0.0\n",
      "FL    83.888079       0.0\n",
      "AB    60.224852       0.0\n",
      "AF    53.517631       0.0\n",
      "CR    50.408764       0.0\n",
      "BQ    49.315472       0.0\n",
      "DI     45.89467       0.0\n",
      "EH    38.219507       0.0\n",
      "FD     36.58523       0.0\n",
      "BC    34.055825       0.0\n",
      "DA    31.591489       0.0\n",
      "DH    29.076517       0.0\n",
      "FE      26.7766       0.0\n",
      "BN    25.466834  0.000001\n",
      "CD    21.570755  0.000004\n",
      "FR    20.684355  0.000007\n",
      "EE    17.124726   0.00004\n",
      "BP    17.013367  0.000042\n",
      "GF    16.174458  0.000065\n",
      "EB    16.099116  0.000068\n",
      "DL     15.07087  0.000115\n",
      "DE    14.020601  0.000198\n",
      "CC    13.439736  0.000268\n",
      "AM    12.424851  0.000455\n",
      "FI    10.052604  0.001597\n",
      "GL     8.809108  0.003114\n",
      "CU      5.38575  0.020628\n"
     ]
    }
   ],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "# BASE_DIR = '/kaggle/input/icr-identify-age-related-conditions/'\n",
    "BASE_DIR = '../../data'\n",
    "train_df = pd.read_csv(f'{BASE_DIR}/train.csv')\n",
    "greeks_df = pd.read_csv(f'{BASE_DIR}/greeks.csv')\n",
    "test_df = pd.read_csv(f'{BASE_DIR}/test.csv')\n",
    "submission_df = pd.read_csv(f'{BASE_DIR}/sample_submission.csv')\n",
    "\n",
    "# å‰å‡¦ç†\n",
    "train_df, test_df = preprocessing_pipeline(train_df, test_df)\n",
    "\n",
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°ã«åˆ†å‰²\n",
    "X_train = train_df.drop(['Id', 'EJ', 'Class'], axis=1)\n",
    "y_train = train_df['Class']\n",
    "y_train.columns = ['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f5345e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n",
      "our out of folds CV score is 0.2904466303611915\n"
     ]
    }
   ],
   "source": [
    "# å„åˆ†å‰²ã”ã¨ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äºˆæ¸¬å€¤ã‚’æ ¼ç´\n",
    "preds = np.zeros(len(test_df.drop([\"Id\", 'EJ'], axis=1)))\n",
    "# å„åˆ†å‰²ã”ã¨ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’æ ¼ç´\n",
    "scores = 0\n",
    "\n",
    "# K-åˆ†å‰²äº¤å·®æ¤œè¨¼(å±¤åŒ–æŠ½å‡ºæ³•)\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # é€²è¡ŒçŠ¶æ³\n",
    "    print('fold: {}'.format(fold+1))\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²\n",
    "    X_train_fold = X_train.iloc[train_index]\n",
    "    y_train_fold = y_train.iloc[train_index]\n",
    "    X_valid_fold = X_train.iloc[valid_index]\n",
    "    y_valid_fold = y_train.iloc[valid_index]\n",
    "    \n",
    "    # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "    # smote = SMOTE(sampling_strategy={0: 408, 1: 408})\n",
    "    # ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "    # X_train_fold, y_train_fold = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ã‚’è¨ˆç®—\n",
    "    train_w0, train_w1 = calc_log_loss_weight(y_train_fold)\n",
    "    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ã‚’è¨ˆç®—\n",
    "    valid_w0, valid_w1 = calc_log_loss_weight(y_valid_fold)\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’xgbç”¨ã«å¤‰æ›\n",
    "    xgb_train = xgb.DMatrix(data=X_train_fold, label=y_train_fold, weight=y_train_fold.map({0: train_w0, 1: train_w1}))\n",
    "    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’xgbç”¨ã«å¤‰æ›\n",
    "    xgb_valid = xgb.DMatrix(data=X_valid_fold, label=y_valid_fold, weight=y_valid_fold.map({0: valid_w0, 1: valid_w1}))\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "    model = xgb.train(\n",
    "        CFG.xgb_params, \n",
    "        dtrain = xgb_train, \n",
    "        num_boost_round = CFG.num_boost_round,\n",
    "        evals = [(xgb_train, 'train'), (xgb_valid, 'eval')], \n",
    "        early_stopping_rounds = CFG.early_stopping_rounds,\n",
    "        verbose_eval = False, # æ•´æ•°ã«è¨­å®šã™ã‚‹ã¨ã€nå›ã”ã¨ã®ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹ãƒ†ãƒ¼ã‚¸ã§è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º\n",
    "    )\n",
    "    # æ¤œè¨¼\n",
    "    valid_preds = model.predict(xgb.DMatrix(X_valid_fold), iteration_range=(0, model.best_ntree_limit))\n",
    "    # äºˆæ¸¬\n",
    "    pred = model.predict(xgb.DMatrix(test_df.drop(['Id', 'EJ'], axis=1)), iteration_range=(0, model.best_ntree_limit))\n",
    "    \n",
    "    # äºˆæ¸¬å€¤ã‚’ãƒ©ãƒ™ãƒ«ã«å¤‰æ›\n",
    "    # pred_labels = np.rint(preds)\n",
    "    # è©•ä¾¡\n",
    "    # val_score = balanced_log_loss(y_valid, pred_labels)\n",
    "    val_score = balanced_log_loss(y_valid_fold, valid_preds)\n",
    "\n",
    "    # äºˆæ¸¬ã‚’ä¿å­˜\n",
    "    preds += pred\n",
    "    # ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜\n",
    "    scores += val_score\n",
    "    \n",
    "# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®å¹³å‡å€¤ã‚’è¨ˆç®—\n",
    "test_pred = preds / CFG.n_folds\n",
    "cv_score = scores /  CFG.n_folds\n",
    "print(f'our out of folds CV score is {cv_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a054954d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.993401</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.993401</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.993401</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.993401</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.993401</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id   class_0   class_1\n",
       "0  00eed32682bb  0.993401  0.006599\n",
       "1  010ebe33f668  0.993401  0.006599\n",
       "2  02fa521e1838  0.993401  0.006599\n",
       "3  040e15f562a2  0.993401  0.006599\n",
       "4  046e85c7cc7f  0.993401  0.006599"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æå‡ºç”¨ã«å€¤ã‚’å¤‰æ›\n",
    "submission = pd.DataFrame(columns = submission_df.columns)\n",
    "submission['Id'] = test_df['Id']\n",
    "submission['class_0'] = 1 - test_pred\n",
    "submission['class_1'] = test_pred\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
