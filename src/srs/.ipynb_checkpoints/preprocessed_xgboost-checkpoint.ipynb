{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52304d70",
   "metadata": {},
   "source": [
    "# preprocessed_xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443f30b",
   "metadata": {},
   "source": [
    "## 実験結果\n",
    "* 何もなし  \n",
    "CV: 0.28495961062567476\n",
    "* 欠損値補完  \n",
    "CV: 0.2937347187995721\n",
    "* DV, AR削除  \n",
    "CV: 0.29048520579559434\n",
    "* Robust Scaler  \n",
    "CV: 0.30632285148599936\n",
    "* 欠損値補完+特徴量抽出+Robust Scaler  \n",
    "CV: 0.30632285148599936  \n",
    "LB: 0.24\n",
    "* 欠損値補完+特徴量抽出+Robust Scaler+外れ値除去  \n",
    "CV: 0.2904466303611915  \n",
    "LB: 0.26\n",
    "## 7/29変更点\n",
    "1. 外れ値除去: 外れ値の上限下限を訓練データとテストデータ別々に計算\n",
    "2. train_integerizedを特徴量に使用\n",
    "* 欠損値補完+特徴量抽出+Robust Scaler+train_integerized+ハイパーパラメータ変更\n",
    "CV: 0.2970240285484198\n",
    "* 欠損値補完+特徴量抽出+Robust Scaler+train_integerized+ハイパーパラメータ変更+fevalをbalanced_log_lossに変更\n",
    "CV: 0.5590221126039339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c565dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE # SMOTE\n",
    "from sklearn.impute import KNNImputer # kNN Imputation\n",
    "from sklearn.feature_selection import SelectKBest, f_classif# Feature Selection\n",
    "# Data Encoder and Scaler\n",
    "import category_encoders as encoders\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a894e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'local'\n",
    "# env = 'kaggle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55269571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "if env == 'local':\n",
    "    BASE_DIR = '../../data'\n",
    "elif env == 'kaggle':\n",
    "    BASE_DIR = '/kaggle/input/icr-identify-age-related-conditions/'\n",
    "else:\n",
    "    raise ValueError(\"Invalid environment. Set env as 'local' or 'kaggle'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c555c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    '''前処理を行うクラス'''\n",
    "    def __init__(self, train_df, test_df):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.numerical_columns = train_df.drop(['Id', 'EJ', 'Class'], axis=1).columns\n",
    "        self.features = pd.DataFrame(index=self.numerical_columns, columns=[\"F_value\", \"p_value\"])\n",
    "        \n",
    "    def knn_imputer(self):\n",
    "        # インスタンス生成\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        \n",
    "        # ローカル変数に値を格納\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "        \n",
    "        # 訓練データに欠損値代入\n",
    "        train_df_imputed = pd.DataFrame(imputer.fit_transform(temp_train_df[self.numerical_columns]), columns=self.numerical_columns)\n",
    "        \n",
    "        # テストデータに欠損値代入\n",
    "        test_df_imputed = pd.DataFrame(imputer.transform(temp_test_df[self.numerical_columns]), columns=self.numerical_columns)\n",
    "\n",
    "        # 元の訓練データも欠損値を補完したデータに置き換える\n",
    "        temp_train_df = temp_train_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_train_df = pd.concat([temp_train_df, train_df_imputed], axis=1)\n",
    "\n",
    "        # テストデータを欠損値を代入したデータに置き換える\n",
    "        temp_test_df = temp_test_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_test_df = pd.concat([temp_test_df, test_df_imputed], axis=1)\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "    \n",
    "    def clip_outliers_ver1(self):\n",
    "        # ローカル変数に値を格納\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "\n",
    "        first_quartiles = temp_train_df[self.numerical_columns].quantile(0.25) # 第１四分位数\n",
    "        third_quartiles = temp_train_df[self.numerical_columns].quantile(0.75) # 第３四分位数\n",
    "        iqr = third_quartiles - first_quartiles # 四分位範囲\n",
    "\n",
    "        lower_bound = first_quartiles - (iqr * 1.5) #外れ値の下限\n",
    "        upper_bound = third_quartiles + (iqr * 1.5) #外れ値の上限\n",
    "\n",
    "        # 訓練データとテストデータの両方に対して処理を行う\n",
    "        for df in [temp_train_df, temp_test_df]:\n",
    "            df[self.numerical_columns] = df[self.numerical_columns].clip(lower_bound, upper_bound, axis=1)\n",
    "\n",
    "        return temp_train_df, temp_test_df\n",
    "    \n",
    "    def clip_outliers(self):\n",
    "        '''訓練データとテストデータで別々に上限、下限を計算するよう変更'''\n",
    "        # ローカル変数に値を格納\n",
    "        clipped_df = []\n",
    "        \n",
    "        for df in [self.train_df, self.test_df]:\n",
    "            temp_df = df\n",
    "            first_quartiles = temp_df[self.numerical_columns].quantile(0.25) # 第１四分位数\n",
    "            third_quartiles = temp_df[self.numerical_columns].quantile(0.75) # 第３四分位数\n",
    "            iqr = third_quartiles - first_quartiles # 四分位範囲\n",
    "\n",
    "            lower_bound = first_quartiles - (iqr * 1.5) #外れ値の下限\n",
    "            upper_bound = third_quartiles + (iqr * 1.5) #外れ値の上限\n",
    "\n",
    "            temp_df[self.numerical_columns] = temp_df[self.numerical_columns].clip(lower_bound, upper_bound, axis=1)\n",
    "            clipped_df.append(temp_df)\n",
    "\n",
    "        return clipped_df[0], clipped_df[1]\n",
    "        \n",
    "    def robust_scaler(self):\n",
    "        # インスタンス生成\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        # ローカル変数に値を格納\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "\n",
    "        '''訓練データのスケーリング'''\n",
    "        # インデックスを抽出\n",
    "        index = temp_train_df.index\n",
    "        # スケーリング\n",
    "        scaler_train = scaler.fit_transform(temp_train_df[self.numerical_columns])\n",
    "        scaled_train_df = pd.DataFrame(scaler_train, columns=self.numerical_columns)\n",
    "        # インデックスを振りなおす\n",
    "        scaled_train_df.index = index\n",
    "\n",
    "        '''テストデータのスケーリング'''\n",
    "        # インデックスを抽出\n",
    "        index = temp_test_df.index\n",
    "        # スケーリング\n",
    "        scaler_test = scaler.fit_transform(temp_test_df[self.numerical_columns])\n",
    "        scaled_test_df = pd.DataFrame(scaler_test, columns=self.numerical_columns)\n",
    "        # インデックスを振りなおす\n",
    "        scaled_test_df.index = index\n",
    "        \n",
    "        # 元の訓練データも欠損値を補完したデータに置き換える\n",
    "        temp_train_df = temp_train_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_train_df = pd.concat([temp_train_df, scaled_train_df], axis=1)\n",
    "\n",
    "        # テストデータを欠損値を代入したデータに置き換える\n",
    "        temp_test_df = temp_test_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_test_df = pd.concat([temp_test_df, scaled_test_df], axis=1)\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "    \n",
    "    def select_k_best(self, pvalue_upper_limit = 0.1, fscore_lower_limit = 5):\n",
    "        # ローカル変数に値を格納\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "        \n",
    "        # 訓練データを説明変数と目的変数に分割\n",
    "        X_train = temp_train_df.drop(['Id', 'EJ', 'Class'], axis=1)\n",
    "        y_train = temp_train_df['Class']\n",
    "        # y_train.columns = ['Class']\n",
    "        '''F値とp値を計算'''\n",
    "        # インスタンス生成\n",
    "        #     回帰: f_regression, mutual_info_regression\n",
    "        #     分類: chi2, f_classif(分散分析のF値), mutual_info_classif\n",
    "        # この時点ではkをもとの訓練データと同じにする\n",
    "        fs = SelectKBest(score_func=f_classif, k=len(X_train.columns))\n",
    "        # 特徴量選択\n",
    "        X_selected = fs.fit_transform(X_train, y_train.values)\n",
    "\n",
    "        '''選択したF値とp値と設定した閾値を用いて特徴量を選択'''\n",
    "        new_features = [] # 選択された特徴量を格納\n",
    "        drop_features = [] # 使わない特徴量を格納\n",
    "\n",
    "        # F値が大きく、p値の小さい特徴量を選択\n",
    "        for i in range(len(X_train.columns)):\n",
    "            # F値とp値を格納\n",
    "            self.features.loc[X_train.columns[i], \"F_value\"] = fs.scores_[i]\n",
    "            self.features.loc[X_train.columns[i], \"p_value\"] = fs.pvalues_[i]\n",
    "            \n",
    "            if fs.pvalues_[i] <= pvalue_upper_limit and fs.scores_[i] >= fscore_lower_limit:\n",
    "                new_features.append(X_train.columns[i])\n",
    "            else:\n",
    "                drop_features.append(X_train.columns[i])\n",
    "\n",
    "        # 訓練データから選択した特徴量を抽出        \n",
    "        X_selected_final = pd.DataFrame(X_selected)\n",
    "        X_selected_final.columns = X_train.columns\n",
    "        X_selected_final = X_selected_final[new_features]\n",
    "        # print('=' * 30)\n",
    "        # print('After the SelectKBest = {}'.format(X_selected_final.shape))\n",
    "        # print('Drop-out Features = {}'.format(len(drop_features)))\n",
    "\n",
    "        # 元のデータに反映\n",
    "        # X_train = X_train.drop(drop_features, axis=1)\n",
    "        temp_train_df = temp_train_df.drop(drop_features, axis=1)\n",
    "        temp_test_df = temp_test_df.drop(drop_features, axis=1)\n",
    "        \n",
    "        self.features = self.features.loc[new_features, :] # 選択された特徴量だけをfeaturesに保存\n",
    "        self.features = self.features.sort_values(\"F_value\", ascending=False)# F値が大きい順にソート\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "        \n",
    "def preprocessing_pipeline(train_df, test_df):\n",
    "    # クラスのインスタンスを生成\n",
    "    preprocessor = Preprocessing(train_df, test_df)\n",
    "    \n",
    "    # 各メソッドを順に実行\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.knn_imputer() # 欠損値代入\n",
    "    # preprocessor.train_df, preprocessor.test_df = preprocessor.clip_outliers() # 外れ値除去\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.robust_scaler() # スケーリング\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.select_k_best(pvalue_upper_limit = 0.1, fscore_lower_limit = 5) # 特徴量選択\n",
    "    \n",
    "    # print('selected features: \\n{}'.format(preprocessor.features))\n",
    "\n",
    "    # 最終的に処理されたデータフレームを返す\n",
    "    return preprocessor.train_df, preprocessor.test_df\n",
    "\n",
    "# 評価基準\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability 𝑝 is replaced with max(min(𝑝,1−10−15),10−15)\n",
    "    y_pred = np.maximum(np.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1-y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "def balanced_log_loss_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    loss = balanced_log_loss(y_true, y_pred)\n",
    "    return 'balanced_log_loss', loss\n",
    "\n",
    "# Classの０，１の割合をそれぞれ計算\n",
    "def calc_log_loss_weight(y_true):\n",
    "    nc = np.bincount(y_true)\n",
    "    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n",
    "    return w0, w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba2e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_training(X_train, y_train, X_valid, y_valid):\n",
    "    # 訓練データの重みを計算\n",
    "    train_w0, train_w1 = calc_log_loss_weight(y_train)\n",
    "    # 検証データの重みを計算\n",
    "    valid_w0, valid_w1 = calc_log_loss_weight(y_valid)\n",
    "    # 訓練データをxgb用に変換\n",
    "    xgb_train = xgb.DMatrix(data=X_train, label=y_train, weight=y_train.map({0: train_w0, 1: train_w1}))\n",
    "    # 検証データをxgb用に変換\n",
    "    xgb_valid = xgb.DMatrix(data=X_valid, label=y_valid, weight=y_valid.map({0: valid_w0, 1: valid_w1}))\n",
    "\n",
    "    # モデルのインスタンス生成\n",
    "    model = xgb.train(\n",
    "        CFG.xgb_params, \n",
    "        dtrain = xgb_train, \n",
    "        num_boost_round = CFG.num_boost_round,\n",
    "        evals = [(xgb_train, 'train'), (xgb_valid, 'eval')], \n",
    "        # feval = balanced_log_loss_eval,\n",
    "        early_stopping_rounds = CFG.early_stopping_rounds,\n",
    "        verbose_eval = False, # 整数に設定すると、n回ごとのブースティングステージで評価メトリクスを表示\n",
    "    )\n",
    "    # 検証\n",
    "    valid_preds = model.predict(xgb.DMatrix(X_valid), iteration_range=(0, model.best_ntree_limit))\n",
    "    \n",
    "    return model, valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0015ff-0100-40af-8dc6-ccf78cd18c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle('params/xgb_best_param.pkl').best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38362eb8-3c17-4953-add7-f8d40914cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    '''設定値を格納'''\n",
    "    num_boost_round = 926\n",
    "    early_stopping_rounds = 98\n",
    "    n_folds = 5 # 公差検証の分割数\n",
    "    n_trials = 100 # ハイパーパラメータチューニングの試行回数\n",
    "    seed = 1234\n",
    "    # xgboost設定値\n",
    "    xgb_params = {\n",
    "        'objective': 'binary:logistic',# 学習タスク\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'random_state': seed,\n",
    "        'learning_rate': 0.01,\n",
    "        'eval_metric': 'rmse',\n",
    "        # 探索したパラメータ\n",
    "        'max_depth': 43,\n",
    "        'colsample_bytree': 0.9270015786178574,\n",
    "        'subsample': 1.0,\n",
    "        'gamma': 0.9008025641267255,\n",
    "        'lambda': 0.3150372040663734,\n",
    "        'min_child_weight': 7,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892075e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "if env == 'local':\n",
    "    BASE_DIR = '../../data'\n",
    "elif env == 'kaggle':\n",
    "    BASE_DIR = '/kaggle/input/icr-identify-age-related-conditions/'\n",
    "else:\n",
    "    raise ValueError(\"Invalid environment. Set env as 'local' or 'kaggle'.\")\n",
    "\n",
    "# train_df = pd.read_csv(f'{BASE_DIR}/train.csv')\n",
    "train_df = pd.read_csv(f'{BASE_DIR}/train_integerized.csv')\n",
    "greeks_df = pd.read_csv(f'{BASE_DIR}/greeks.csv')\n",
    "test_df = pd.read_csv(f'{BASE_DIR}/test.csv')\n",
    "submission_df = pd.read_csv(f'{BASE_DIR}/sample_submission.csv')\n",
    "\n",
    "# 前処理\n",
    "train_df, test_df = preprocessing_pipeline(train_df, test_df)\n",
    "\n",
    "# 訓練データを説明変数と目的変数に分割\n",
    "X_train = train_df.drop(['Id', 'EJ', 'Class'], axis=1)\n",
    "y_train = train_df['Class']\n",
    "y_train.columns = ['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f5345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n",
      "our out of folds CV score is 0.29338396162110936\n"
     ]
    }
   ],
   "source": [
    "# 各分割ごとのバリデーションスコアを格納\n",
    "scores = 0\n",
    "# モデルを保存\n",
    "models = []\n",
    "\n",
    "# K-分割交差検証(層化抽出法)\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # 進行状況\n",
    "    print('fold: {}'.format(fold+1))\n",
    "    # 訓練データを分割\n",
    "    X_train_fold = X_train.iloc[train_index]\n",
    "    y_train_fold = y_train.iloc[train_index]\n",
    "    X_valid_fold = X_train.iloc[valid_index]\n",
    "    y_valid_fold = y_train.iloc[valid_index]\n",
    "    \n",
    "    # モデルを訓練、予測を出力\n",
    "    model, valid_preds = xgb_training(X_train_fold, y_train_fold, X_valid_fold, y_valid_fold)\n",
    "\n",
    "    # 評価\n",
    "    val_score = balanced_log_loss(y_valid_fold, valid_preds)\n",
    "    # スコアを保存\n",
    "    scores += val_score\n",
    "    # モデルを保存\n",
    "    models.append(model)\n",
    "    \n",
    "# クロスバリデーションの平均値を計算\n",
    "cv_score = scores /  CFG.n_folds\n",
    "print(f'our out of folds CV score is {cv_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28a97355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測\n",
    "# 各分割ごとのテストデータに対する予測値を格納\n",
    "preds = np.zeros(len(test_df.drop([\"Id\", 'EJ'], axis=1)))\n",
    "for i in range(len(models)):\n",
    "    pred = models[i].predict(xgb.DMatrix(test_df.drop(['Id', 'EJ'], axis=1)), iteration_range=(0, models[i].best_iteration))\n",
    "    preds += pred\n",
    "test_pred = preds / CFG.n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a054954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出用に値を変換\n",
    "if env == 'kaggle':\n",
    "    submission = pd.DataFrame(columns = submission_df.columns)\n",
    "    submission['Id'] = test_df['Id']\n",
    "    submission['class_0'] = 1 - test_pred\n",
    "    submission['class_1'] = test_pred\n",
    "    submission.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
