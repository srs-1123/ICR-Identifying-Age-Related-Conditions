{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52304d70",
   "metadata": {},
   "source": [
    "# xgb+lgb+tabpfn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443f30b",
   "metadata": {},
   "source": [
    "## 実験結果\n",
    "* xgb(特徴量抽出)  \n",
    "CV: 0.3058027696858661\n",
    "* lgb(特徴量抽出)  \n",
    "CV: 0.31445132158950145\n",
    "* TabPFN(特徴量抽出)  \n",
    "CV: 0.430984415297508"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c2bf9-2843-447c-bfea-2140254115f3",
   "metadata": {},
   "source": [
    "## TabPFNのインストール\n",
    "* 事前にダウンロードするファイル: <https://www.kaggle.com/datasets/carlmcbrideellis/tabpfn-019-whl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d6f7c16-2dc2-40db-8dd9-c565c4348d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q /kaggle/input/tabpfn-019-whl/tabpfn-0.1.9-py3-none-any.whl\n",
    "# !mkdir /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n",
    "# !cp /kaggle/input/tabpfn-019-whl/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c565dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from tabpfn import TabPFNClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE # SMOTE\n",
    "from sklearn.impute import KNNImputer # kNN Imputation\n",
    "from sklearn.feature_selection import SelectKBest, f_classif# Feature Selection\n",
    "# Data Encoder and Scaler\n",
    "import category_encoders as encoders\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# 環境を指定\n",
    "env = 'local'\n",
    "# env = 'kaggle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b138da54-574e-492d-8e40-e463f9c6be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_imputer(train_df, test_df):\n",
    "    # インスタンス生成\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "    numerical_columns = train_df.drop(['Id', 'EJ', 'Class'], axis=1).columns\n",
    "\n",
    "    # 訓練データに欠損値代入\n",
    "    train_df_imputed = pd.DataFrame(imputer.fit_transform(train_df[numerical_columns]), columns=numerical_columns)\n",
    "    # テストデータに欠損値代入\n",
    "    test_df_imputed = pd.DataFrame(imputer.transform(test_df[numerical_columns]), columns=numerical_columns)\n",
    "\n",
    "    # 元の訓練データも欠損値を補完したデータに置き換える\n",
    "    train_df = train_df.drop(numerical_columns, axis=1)\n",
    "    train_df = pd.concat([train_df, train_df_imputed], axis=1)\n",
    "\n",
    "    # テストデータを欠損値を代入したデータに置き換える\n",
    "    test_df = test_df.drop(numerical_columns, axis=1)\n",
    "    test_df = pd.concat([test_df, test_df_imputed], axis=1)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "def select_k_best(train_df, test_df, pvalue_upper_limit = 0.1, fscore_lower_limit = 5):\n",
    "    # 欠損値の補完\n",
    "    train_df, test_df = knn_imputer(train_df, test_df)\n",
    "    # 数値データの列\n",
    "    numerical_columns = train_df.drop(['Id', 'EJ', 'Class'], axis=1).columns\n",
    "    features = pd.DataFrame(index=numerical_columns, columns=[\"F_value\", \"p_value\"])\n",
    "    # 訓練データを説明変数と目的変数に分割\n",
    "    X_train = train_df.drop(['Id', 'EJ', 'Class'], axis=1)\n",
    "    y_train = train_df['Class']\n",
    "    # y_train.columns = ['Class']\n",
    "    '''F値とp値を計算'''\n",
    "    # インスタンス生成\n",
    "    #     回帰: f_regression, mutual_info_regression\n",
    "    #     分類: chi2, f_classif(分散分析のF値), mutual_info_classif\n",
    "    # この時点ではkをもとの訓練データと同じにする\n",
    "    fs = SelectKBest(score_func=f_classif, k=len(X_train.columns))\n",
    "    # 特徴量選択\n",
    "    X_selected = fs.fit_transform(X_train, y_train.values)\n",
    "\n",
    "    '''選択したF値とp値と設定した閾値を用いて特徴量を選択'''\n",
    "    new_features = [] # 選択された特徴量を格納\n",
    "    drop_features = [] # 使わない特徴量を格納\n",
    "\n",
    "    # F値が大きく、p値の小さい特徴量を選択\n",
    "    for i in range(len(X_train.columns)):\n",
    "        # F値とp値を格納\n",
    "        features.loc[X_train.columns[i], \"F_value\"] = fs.scores_[i]\n",
    "        features.loc[X_train.columns[i], \"p_value\"] = fs.pvalues_[i]\n",
    "        \n",
    "        if fs.pvalues_[i] <= pvalue_upper_limit and fs.scores_[i] >= fscore_lower_limit:\n",
    "            new_features.append(X_train.columns[i])\n",
    "        else:\n",
    "            drop_features.append(X_train.columns[i])\n",
    "    \n",
    "    features = features.loc[new_features, :] # 選択された特徴量だけをfeaturesに保存\n",
    "    features = features.sort_values(\"F_value\", ascending=False)# F値が大きい順にソート\n",
    "    \n",
    "    return features, drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c555c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    '''前処理を行うクラス'''\n",
    "    def __init__(self, train_df, test_df):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.numerical_columns = train_df.drop(['Id', 'EJ', 'Class'], axis=1).columns\n",
    "        self.features = pd.DataFrame(index=self.numerical_columns, columns=[\"F_value\", \"p_value\"])\n",
    "        \n",
    "    def knn_imputer(self):\n",
    "        # インスタンス生成\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        \n",
    "        # ローカル変数に値を格納\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "        \n",
    "        # 訓練データに欠損値代入\n",
    "        train_df_imputed = pd.DataFrame(imputer.fit_transform(temp_train_df[self.numerical_columns]), columns=self.numerical_columns)\n",
    "        \n",
    "        # テストデータに欠損値代入\n",
    "        test_df_imputed = pd.DataFrame(imputer.transform(temp_test_df[self.numerical_columns]), columns=self.numerical_columns)\n",
    "\n",
    "        # 元の訓練データも欠損値を補完したデータに置き換える\n",
    "        temp_train_df = temp_train_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_train_df = pd.concat([temp_train_df, train_df_imputed], axis=1)\n",
    "\n",
    "        # テストデータを欠損値を代入したデータに置き換える\n",
    "        temp_test_df = temp_test_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_test_df = pd.concat([temp_test_df, test_df_imputed], axis=1)\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "    \n",
    "    def clip_outliers_ver1(self):\n",
    "        # ローカル変数に値を格納\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "\n",
    "        first_quartiles = temp_train_df[self.numerical_columns].quantile(0.25) # 第１四分位数\n",
    "        third_quartiles = temp_train_df[self.numerical_columns].quantile(0.75) # 第３四分位数\n",
    "        iqr = third_quartiles - first_quartiles # 四分位範囲\n",
    "\n",
    "        lower_bound = first_quartiles - (iqr * 1.5) #外れ値の下限\n",
    "        upper_bound = third_quartiles + (iqr * 1.5) #外れ値の上限\n",
    "\n",
    "        # 訓練データとテストデータの両方に対して処理を行う\n",
    "        for df in [temp_train_df, temp_test_df]:\n",
    "            df[self.numerical_columns] = df[self.numerical_columns].clip(lower_bound, upper_bound, axis=1)\n",
    "\n",
    "        return temp_train_df, temp_test_df\n",
    "    \n",
    "    def clip_outliers(self):\n",
    "        '''訓練データとテストデータで別々に上限、下限を計算するよう変更'''\n",
    "        # ローカル変数に値を格納\n",
    "        clipped_df = []\n",
    "        \n",
    "        for df in [self.train_df, self.test_df]:\n",
    "            temp_df = df\n",
    "            first_quartiles = temp_df[self.numerical_columns].quantile(0.25) # 第１四分位数\n",
    "            third_quartiles = temp_df[self.numerical_columns].quantile(0.75) # 第３四分位数\n",
    "            iqr = third_quartiles - first_quartiles # 四分位範囲\n",
    "\n",
    "            lower_bound = first_quartiles - (iqr * 1.5) #外れ値の下限\n",
    "            upper_bound = third_quartiles + (iqr * 1.5) #外れ値の上限\n",
    "\n",
    "            temp_df[self.numerical_columns] = temp_df[self.numerical_columns].clip(lower_bound, upper_bound, axis=1)\n",
    "            clipped_df.append(temp_df)\n",
    "\n",
    "        return clipped_df[0], clipped_df[1]\n",
    "        \n",
    "    def robust_scaler(self):\n",
    "        # インスタンス生成\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        # ローカル変数に値を格納\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "\n",
    "        '''訓練データのスケーリング'''\n",
    "        # インデックスを抽出\n",
    "        index = temp_train_df.index\n",
    "        # スケーリング\n",
    "        scaler_train = scaler.fit_transform(temp_train_df[self.numerical_columns])\n",
    "        scaled_train_df = pd.DataFrame(scaler_train, columns=self.numerical_columns)\n",
    "        # インデックスを振りなおす\n",
    "        scaled_train_df.index = index\n",
    "\n",
    "        '''テストデータのスケーリング'''\n",
    "        # インデックスを抽出\n",
    "        index = temp_test_df.index\n",
    "        # スケーリング\n",
    "        scaler_test = scaler.fit_transform(temp_test_df[self.numerical_columns])\n",
    "        scaled_test_df = pd.DataFrame(scaler_test, columns=self.numerical_columns)\n",
    "        # インデックスを振りなおす\n",
    "        scaled_test_df.index = index\n",
    "        \n",
    "        # 元の訓練データも欠損値を補完したデータに置き換える\n",
    "        temp_train_df = temp_train_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_train_df = pd.concat([temp_train_df, scaled_train_df], axis=1)\n",
    "\n",
    "        # テストデータを欠損値を代入したデータに置き換える\n",
    "        temp_test_df = temp_test_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_test_df = pd.concat([temp_test_df, scaled_test_df], axis=1)\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "        \n",
    "def preprocessing_pipeline(train_df, test_df):\n",
    "    # クラスのインスタンスを生成\n",
    "    preprocessor = Preprocessing(train_df, test_df)\n",
    "    \n",
    "    # 各メソッドを順に実行\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.knn_imputer() # 欠損値代入\n",
    "    # preprocessor.train_df, preprocessor.test_df = preprocessor.clip_outliers() # 外れ値除去\n",
    "    # preprocessor.train_df, preprocessor.test_df = preprocessor.robust_scaler() # スケーリング\n",
    "    \n",
    "    # print('selected features: \\n{}'.format(preprocessor.features))\n",
    "\n",
    "    # 最終的に処理されたデータフレームを返す\n",
    "    return preprocessor.train_df, preprocessor.test_df\n",
    "\n",
    "# 評価基準\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability 𝑝 is replaced with max(min(𝑝,1−10−15),10−15)\n",
    "    y_pred = np.maximum(np.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1-y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "def balanced_log_loss_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    loss = balanced_log_loss(y_true, y_pred)\n",
    "    return 'balanced_log_loss', loss\n",
    "\n",
    "# Classの０，１の割合をそれぞれ計算\n",
    "def calc_log_loss_weight(y_true):\n",
    "    nc = np.bincount(y_true)\n",
    "    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n",
    "    return w0, w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba2e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_training(X_train, y_train, X_valid, y_valid):\n",
    "    # 訓練データの重みを計算\n",
    "    train_w0, train_w1 = calc_log_loss_weight(y_train)\n",
    "    # 検証データの重みを計算\n",
    "    valid_w0, valid_w1 = calc_log_loss_weight(y_valid)\n",
    "    # 訓練データをxgb用に変換\n",
    "    xgb_train = xgb.DMatrix(data=X_train, label=y_train, weight=y_train.map({0: train_w0, 1: train_w1}))\n",
    "    # 検証データをxgb用に変換\n",
    "    xgb_valid = xgb.DMatrix(data=X_valid, label=y_valid, weight=y_valid.map({0: valid_w0, 1: valid_w1}))\n",
    "\n",
    "    # モデルのインスタンス生成\n",
    "    model = xgb.train(\n",
    "        CFG.xgb_params, \n",
    "        dtrain = xgb_train, \n",
    "        num_boost_round = CFG.num_boost_round,\n",
    "        evals = [(xgb_train, 'train'), (xgb_valid, 'eval')], \n",
    "        early_stopping_rounds = CFG.early_stopping_rounds,\n",
    "        verbose_eval = False, # 整数に設定すると、n回ごとのブースティングステージで評価メトリクスを表示\n",
    "    )\n",
    "    # 検証\n",
    "    valid_preds = model.predict(xgb.DMatrix(X_valid), iteration_range=(0, model.best_ntree_limit))\n",
    "    \n",
    "    return model, valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95208b27-753f-4a80-872c-13e0c963350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_training(X_train, y_train, X_valid, y_valid):\n",
    "    # 訓練データの重みを計算\n",
    "    train_w0, train_w1 = calc_log_loss_weight(y_train)\n",
    "    # 検証データの重みを計算\n",
    "    valid_w0, valid_w1 = calc_log_loss_weight(y_valid)\n",
    "    # 訓練データをlgb用に変換\n",
    "    lgb_train = lgb.Dataset(X_train, y_train, weight=y_train.map({0: train_w0, 1: train_w1}))\n",
    "    # 検証データをlgb用に変換\n",
    "    lgb_valid = lgb.Dataset(X_valid, y_valid, weight=y_valid.map({0: valid_w0, 1: valid_w1}))\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params = CFG.lgb_params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = CFG.num_boost_round,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = CFG.early_stopping_rounds,\n",
    "        verbose_eval = False,\n",
    "    )\n",
    "    # 検証\n",
    "    valid_preds = model.predict(X_valid)\n",
    "    \n",
    "    return model, valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49b5c703-444c-4c8c-944a-58b7d44e5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabpfn_training(X_train, y_train, X_valid, y_valid):\n",
    "    # インスタンス生成\n",
    "    model = TabPFNClassifier(device='cpu', N_ensemble_configurations=256)\n",
    "    # 学習\n",
    "    model.fit(X_train, y_train)\n",
    "    # 検証\n",
    "    valid_preds = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    return model, valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb0015ff-0100-40af-8dc6-ccf78cd18c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle('params/lgb_best_param.pkl').best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38362eb8-3c17-4953-add7-f8d40914cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    '''設定値を格納'''\n",
    "    num_boost_round = 926\n",
    "    early_stopping_rounds = 98\n",
    "    n_folds = 5 # 公差検証の分割数\n",
    "    n_trials = 100 # ハイパーパラメータチューニングの試行回数\n",
    "    seed = 1234\n",
    "    learning_rate = 0.01\n",
    "    boosting_type = \"dart\"\n",
    "    # xgboost設定値\n",
    "    xgb_params = {\n",
    "        'objective': 'binary:logistic',# 学習タスク\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'random_state': seed,\n",
    "        'learning_rate': learning_rate,\n",
    "        'eval_metric': 'rmse',\n",
    "        # 探索したパラメータ\n",
    "        'max_depth': 43,\n",
    "        'colsample_bytree': 0.9270015786178574,\n",
    "        'subsample': 1.0,\n",
    "        'gamma': 0.9008025641267255,\n",
    "        'lambda': 0.3150372040663734,\n",
    "        'min_child_weight': 7,\n",
    "    }\n",
    "    # light-gbm設定値\n",
    "    lgb_params = {\n",
    "        # 探索するパラメータ\n",
    "        'verbosity': -1, # 学習途中の情報を表示するかどうか\n",
    "        \"lambda_l1\": 1.0760072734927809e-05,\n",
    "        \"lambda_l2\": 0.17928637029753666,\n",
    "        \"num_leaves\": 152,\n",
    "        \"feature_fraction\": 0.6553649132473736,\n",
    "        \"bagging_fraction\": 0.20105587614468057,\n",
    "        \"min_child_samples\": 25,\n",
    "        \n",
    "        # 固定値\n",
    "        \"boosting_type\": boosting_type,\n",
    "        \"objective\": \"binary\",\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        'seed': seed,\n",
    "        # 'n_jobs': -1, # -1でコア数をマックスで使う\n",
    "        'is_unbalance':True, # 不均衡データの場合にTrueにする\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "892075e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "if env == 'local':\n",
    "    BASE_DIR = '../../data'\n",
    "elif env == 'kaggle':\n",
    "    BASE_DIR = '/kaggle/input/icr-identify-age-related-conditions/'\n",
    "else:\n",
    "    raise ValueError(\"Invalid environment. Set env as 'local' or 'kaggle'.\")\n",
    "\n",
    "train_df = pd.read_csv(f'{BASE_DIR}/train.csv')\n",
    "# train_df = pd.read_csv(f'{BASE_DIR}/train_integerized.csv')\n",
    "greeks_df = pd.read_csv(f'{BASE_DIR}/greeks.csv')\n",
    "test_df = pd.read_csv(f'{BASE_DIR}/test.csv')\n",
    "submission_df = pd.read_csv(f'{BASE_DIR}/sample_submission.csv')\n",
    "\n",
    "# 前処理\n",
    "# features, drop_features = select_k_best(train_df, test_df, pvalue_upper_limit = 0.1, fscore_lower_limit = 5)\n",
    "features, drop_features = select_k_best(train_df, test_df, pvalue_upper_limit = 0.01, fscore_lower_limit = 3)\n",
    "train_df = train_df.drop(drop_features, axis=1)\n",
    "test_df = test_df.drop(drop_features, axis=1)\n",
    "\n",
    "# 訓練データを説明変数と目的変数に分割\n",
    "X_train = train_df.drop(['Id', 'EJ', 'Class'], axis=1)\n",
    "y_train = train_df['Class']\n",
    "y_train.columns = ['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6662a8aa-00fe-46f3-a266-1f2e738a94e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n",
      "xgb our out of folds CV score is 0.3058027696858661\n"
     ]
    }
   ],
   "source": [
    "# 各分割ごとのバリデーションスコアを格納\n",
    "scores = 0\n",
    "# モデルを保存\n",
    "models = []\n",
    "\n",
    "# K-分割交差検証(層化抽出法)\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # 進行状況\n",
    "    print('fold: {}'.format(fold+1))\n",
    "    # 訓練データを分割\n",
    "    X_train_fold = X_train.iloc[train_index]\n",
    "    y_train_fold = y_train.iloc[train_index]\n",
    "    X_valid_fold = X_train.iloc[valid_index]\n",
    "    y_valid_fold = y_train.iloc[valid_index]\n",
    "    \n",
    "    # モデルを訓練、予測を出力\n",
    "    model, valid_preds = xgb_training(X_train_fold, y_train_fold, X_valid_fold, y_valid_fold)\n",
    "\n",
    "    # 評価\n",
    "    val_score = balanced_log_loss(y_valid_fold, valid_preds)\n",
    "    # スコアを保存\n",
    "    scores += val_score\n",
    "    # モデルを保存\n",
    "    models.append(model)\n",
    "    \n",
    "# クロスバリデーションの平均値を計算\n",
    "cv_score = scores /  CFG.n_folds\n",
    "print(f'xgb our out of folds CV score is {cv_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47f5345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n",
      "lgb our out of folds CV score is 0.31445132158950145\n"
     ]
    }
   ],
   "source": [
    "# 各分割ごとのバリデーションスコアを格納\n",
    "scores = 0\n",
    "# モデルを保存\n",
    "models = []\n",
    "\n",
    "# K-分割交差検証(層化抽出法)\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # 進行状況\n",
    "    print('fold: {}'.format(fold+1))\n",
    "    # 訓練データを分割\n",
    "    X_train_fold = X_train.iloc[train_index]\n",
    "    y_train_fold = y_train.iloc[train_index]\n",
    "    X_valid_fold = X_train.iloc[valid_index]\n",
    "    y_valid_fold = y_train.iloc[valid_index]\n",
    "    \n",
    "    # モデルを訓練、予測を出力\n",
    "    model, valid_preds = lgb_training(X_train_fold, y_train_fold, X_valid_fold, y_valid_fold)\n",
    "\n",
    "    # 評価\n",
    "    val_score = balanced_log_loss(y_valid_fold, valid_preds)\n",
    "    # スコアを保存\n",
    "    scores += val_score\n",
    "    # モデルを保存\n",
    "    models.append(model)\n",
    "    \n",
    "# クロスバリデーションの平均値を計算\n",
    "cv_score = scores /  CFG.n_folds\n",
    "print(f'lgb our out of folds CV score is {cv_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db1fbadb-e52c-45a9-ab76-c8ed47284c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 2\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 3\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 4\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "fold: 5\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "TabPFN our out of folds CV score is 0.430984415297508\n"
     ]
    }
   ],
   "source": [
    "# 各分割ごとのバリデーションスコアを格納\n",
    "scores = 0\n",
    "# モデルを保存\n",
    "models = []\n",
    "\n",
    "# K-分割交差検証(層化抽出法)\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # 進行状況\n",
    "    print('fold: {}'.format(fold+1))\n",
    "    # 訓練データを分割\n",
    "    X_train_fold = X_train.iloc[train_index]\n",
    "    y_train_fold = y_train.iloc[train_index]\n",
    "    X_valid_fold = X_train.iloc[valid_index]\n",
    "    y_valid_fold = y_train.iloc[valid_index]\n",
    "    \n",
    "    # モデルを訓練、予測を出力\n",
    "    model, valid_preds = tabpfn_training(X_train_fold, y_train_fold, X_valid_fold, y_valid_fold)\n",
    "\n",
    "    # 評価\n",
    "    val_score = balanced_log_loss(y_valid_fold, valid_preds)\n",
    "    # スコアを保存\n",
    "    scores += val_score\n",
    "    # モデルを保存\n",
    "    models.append(model)\n",
    "    \n",
    "# クロスバリデーションの平均値を計算\n",
    "cv_score = scores /  CFG.n_folds\n",
    "print(f'TabPFN our out of folds CV score is {cv_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a054954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出用に値を変換\n",
    "if env == 'kaggle':\n",
    "    # 予測\n",
    "    # 各分割ごとのテストデータに対する予測値を格納\n",
    "    preds = np.zeros(len(test_df.drop([\"Id\", 'EJ'], axis=1)))\n",
    "    for i in range(len(models)):\n",
    "        # pred = models[i].predict(xgb.DMatrix(test_df.drop(['Id', 'EJ'], axis=1)), iteration_range=(0, models[i].best_iteration))\n",
    "        pred = models[i].predict(test_df.drop(['Id', 'EJ'], axis=1))\n",
    "        preds += pred\n",
    "    test_pred = preds / CFG.n_folds\n",
    "\n",
    "    # 提出\n",
    "    submission = pd.DataFrame(columns = submission_df.columns)\n",
    "    submission['Id'] = test_df['Id']\n",
    "    submission['class_0'] = 1 - test_pred\n",
    "    submission['class_1'] = test_pred\n",
    "    submission.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
