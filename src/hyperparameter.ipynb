{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c565dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "effab245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šå€¤\n",
    "class CFG:\n",
    "    # å¤‰æ›´ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    n_folds = 5 # å…¬å·®æ¤œè¨¼ã®åˆ†å‰²æ•°(å¤šãã¦20)\n",
    "    n_trials = 50 # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®è©¦è¡Œå›æ•°(100)\n",
    "    # device_type = \"cpu\"\n",
    "    device_type = \"gpu\"\n",
    "    boosting_type = \"gbdt\"\n",
    "    # boosting_type = \"dart\"\n",
    "    \n",
    "    \n",
    "    # ãã®ä»–è¨­å®šå€¤\n",
    "    learning_rate = 0.01\n",
    "    seed = 3407 \n",
    "    target_col = 'Class'\n",
    "    num_boost_round = 50500\n",
    "    early_stopping_round = 300\n",
    "    verbose_eval = 0  # ã“ã®æ•°å­—ã‚’1ã«ã™ã‚‹ã¨å­¦ç¿’æ™‚ã®ã‚¹ã‚³ã‚¢æ¨ç§»ãŒã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³è¡¨ç¤ºã•ã‚Œã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5341650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "test_df[CFG.target_col] = -1\n",
    "submission_df = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "all_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f928f11",
   "metadata": {},
   "source": [
    "BC, CLã¯ã„ã‚‰ã‚“ã‹ã‚‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6800fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN',\n",
    "       'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CR', 'CS',\n",
    "       'CU', 'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY',\n",
    "       'EB', 'EE', 'EG', 'EH', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI',\n",
    "       'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL']\n",
    "categorical_features = ['EJ']\n",
    "features = numerical_features + categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910afd9",
   "metadata": {},
   "source": [
    "### balanced loglossã®è¨ˆç®—ï¼ˆå­¦ç¿’ã§ä½¿ã†ï¼Ÿï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0721bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰å‡¦ç†\n",
    "def Preprocessing(input_df: pd.DataFrame)->pd.DataFrame:\n",
    "    output_df = input_df.copy()\n",
    "    output_df['EJ'] = input_df['EJ'].replace({'A': 0, 'B': 1})\n",
    "    return output_df\n",
    "\n",
    "all_df = Preprocessing(all_df)\n",
    "\n",
    "train_df = all_df[all_df[CFG.target_col] != -1].copy()\n",
    "test_df = all_df[all_df[CFG.target_col] == -1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8a5b023e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# é‡ã¿è¨ˆç®—\\ndef calc_log_loss_weight(y_true):\\n    nc = np.bincount(y_true)\\n    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\\n    return w0, w1\\n'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è©•ä¾¡åŸºæº–\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability ğ‘ is replaced with max(min(ğ‘,1âˆ’10âˆ’15),10âˆ’15)\n",
    "    y_pred = np.maximum(np.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1-y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "\"\"\"\n",
    "# é‡ã¿è¨ˆç®—\n",
    "def calc_log_loss_weight(y_true):\n",
    "    nc = np.bincount(y_true)\n",
    "    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n",
    "    return w0, w1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "47f5345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # light-gbmè¨­å®šå€¤\n",
    "    lgb_params = {\n",
    "        # æ¢ç´¢ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        'verbosity': -1, # å­¦ç¿’é€”ä¸­ã®æƒ…å ±ã‚’è¡¨ç¤ºã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 1.0),\n",
    "        # \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \n",
    "        # å›ºå®šå€¤\n",
    "        \"boosting_type\": CFG.boosting_type,\n",
    "        \"device_type\": CFG.device_type,\n",
    "        \"objective\": \"binary\",\n",
    "        \"learning_rate\": CFG.learning_rate,\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        'seed': CFG.seed,\n",
    "        'n_jobs': -1, # -1ã§ã‚³ã‚¢æ•°ã‚’ãƒãƒƒã‚¯ã‚¹ã§ä½¿ã†\n",
    "        'is_unbalance':True, # ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã«Trueã«ã™ã‚‹\n",
    "    }\n",
    "    \n",
    "    scores = []\n",
    "    # K-åˆ†å‰²äº¤å·®æ¤œè¨¼(å±¤åŒ–æŠ½å‡ºæ³•)\n",
    "    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "    pbar = tqdm(enumerate(kfold.split(train_df, train_df[CFG.target_col])))\n",
    "    for fold, (train_index, valid_index) in pbar:\n",
    "    # for fold, (train_index, valid_index) in enumerate(kfold.split(train_df, train_df[CFG.target_col])):\n",
    "        # ç¾åœ¨ã®è©¦è¡Œå›æ•°ã‚’å‡ºåŠ›\n",
    "        pbar.set_description(\"[train] trials {}\".format(trial.number+1))\n",
    "        \n",
    "        x_train = train_df[features].iloc[train_index]\n",
    "        y_train = train_df[CFG.target_col].iloc[train_index]\n",
    "        x_valid = train_df[features].iloc[valid_index]\n",
    "        y_valid = train_df[CFG.target_col].iloc[valid_index]\n",
    "        \n",
    "        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ã‚’è¨ˆç®—\n",
    "        # train_w0, train_w1 = calc_log_loss_weight(y_train)\n",
    "        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ã‚’è¨ˆç®—\n",
    "        # valid_w0, valid_w1 = calc_log_loss_weight(y_valid)\n",
    "        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’lgbç”¨ã«å¤‰æ›\n",
    "        # lgb_train = lgb.Dataset(x_train, y_train, weight=y_train.map({0: train_w0, 1: train_w1}), categorical_feature=categorical_features)\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=categorical_features)\n",
    "        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’lgbç”¨ã«å¤‰æ›\n",
    "        # lgb_valid = lgb.Dataset(x_valid, y_valid, weight=y_valid.map({0: valid_w0, 1: valid_w1}), categorical_feature=categorical_features)\n",
    "        lgb_valid = lgb.Dataset(x_valid, y_valid, categorical_feature=categorical_features)\n",
    "        \n",
    "        model = lgb.train(\n",
    "                    params = lgb_params,\n",
    "                    train_set = lgb_train,\n",
    "                    num_boost_round = CFG.num_boost_round,\n",
    "                    valid_sets = [lgb_train, lgb_valid],\n",
    "                    early_stopping_rounds = CFG.early_stopping_round,\n",
    "                    verbose_eval = CFG.verbose_eval,\n",
    "                    # å­¦ç¿’æ®µéšã§balanced_log_lossã‚’ä½¿ã†å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆå¤–ã™\n",
    "                    # feval = lgb_metric,\n",
    "                )\n",
    "        # äºˆæ¸¬\n",
    "        preds = model.predict(x_valid)\n",
    "        # äºˆæ¸¬å€¤ã‚’ãƒ©ãƒ™ãƒ«ã«å¤‰æ›\n",
    "        # pred_labels = np.rint(preds)\n",
    "        # è©•ä¾¡\n",
    "        # val_score = balanced_log_loss(y_valid, pred_labels)\n",
    "        val_score = balanced_log_loss(y_valid, preds)\n",
    "        \n",
    "        scores.append(val_score)\n",
    "    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®å¹³å‡å€¤ã‚’è¨ˆç®—\n",
    "    mean_score = np.mean(scores)\n",
    "    \n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bfc7ad2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-27 16:49:44,724]\u001b[0m A new study created in memory with name: no-name-b3a88f1f-ec5a-409e-a334-499d33f17756\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "[train] trials 1: : 0it [00:00, ?it/s]\n",
      "\u001b[33m[W 2023-05-27 16:49:44,757]\u001b[0m Trial 0 failed with parameters: {'lambda_l1': 4.0112881407220285e-06, 'lambda_l2': 1.1606998172727898, 'num_leaves': 172, 'feature_fraction': 0.44241719664617085, 'bagging_fraction': 0.2886148019933221, 'min_child_samples': 100} because of the following error: LightGBMError('GPU Tree Learner was not enabled in this build.\\nPlease recompile with CMake option -DUSE_GPU=1').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eringi/.pyenv/versions/3.10.8/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_2792/1657580388.py\", line 50, in objective\n",
      "    model = lgb.train(\n",
      "  File \"/home/eringi/.pyenv/versions/3.10.8/lib/python3.10/site-packages/lightgbm/engine.py\", line 271, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"/home/eringi/.pyenv/versions/3.10.8/lib/python3.10/site-packages/lightgbm/basic.py\", line 2610, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"/home/eringi/.pyenv/versions/3.10.8/lib/python3.10/site-packages/lightgbm/basic.py\", line 125, in _safe_call\n",
      "    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))\n",
      "lightgbm.basic.LightGBMError: GPU Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_GPU=1\n",
      "\u001b[33m[W 2023-05-27 16:49:44,757]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb ã‚»ãƒ« 10\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mn_trials)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     _optimize(\n\u001b[1;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    435\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32m/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb ã‚»ãƒ« 10\u001b[0m in \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’lgbç”¨ã«å¤‰æ›\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# lgb_valid = lgb.Dataset(x_valid, y_valid, weight=y_valid.map({0: valid_w0, 1: valid_w1}), categorical_feature=categorical_features)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m lgb_valid \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(x_valid, y_valid, categorical_feature\u001b[39m=\u001b[39mcategorical_features)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m model \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m             params \u001b[39m=\u001b[39;49m lgb_params,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m             train_set \u001b[39m=\u001b[39;49m lgb_train,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m             num_boost_round \u001b[39m=\u001b[39;49m CFG\u001b[39m.\u001b[39;49mnum_boost_round,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m             valid_sets \u001b[39m=\u001b[39;49m [lgb_train, lgb_valid],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m             early_stopping_rounds \u001b[39m=\u001b[39;49m CFG\u001b[39m.\u001b[39;49mearly_stopping_round,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m             verbose_eval \u001b[39m=\u001b[39;49m CFG\u001b[39m.\u001b[39;49mverbose_eval,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m             \u001b[39m# å­¦ç¿’æ®µéšã§balanced_log_lossã‚’ä½¿ã†å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆå¤–ã™\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m             \u001b[39m# feval = lgb_metric,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# äºˆæ¸¬\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/eringi/kaggle/kaggle/notebooks/hyperparameter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_valid)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/lightgbm/engine.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39m# construct booster\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     booster \u001b[39m=\u001b[39m Booster(params\u001b[39m=\u001b[39;49mparams, train_set\u001b[39m=\u001b[39;49mtrain_set)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m is_valid_contain_train:\n\u001b[1;32m    273\u001b[0m         booster\u001b[39m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/lightgbm/basic.py:2610\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[1;32m   2608\u001b[0m params_str \u001b[39m=\u001b[39m param_dict_to_str(params)\n\u001b[1;32m   2609\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_void_p()\n\u001b[0;32m-> 2610\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterCreate(\n\u001b[1;32m   2611\u001b[0m     train_set\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   2612\u001b[0m     c_str(params_str),\n\u001b[1;32m   2613\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle)))\n\u001b[1;32m   2614\u001b[0m \u001b[39m# save reference to data\u001b[39;00m\n\u001b[1;32m   2615\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_set \u001b[39m=\u001b[39m train_set\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/lightgbm/basic.py:125\u001b[0m, in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m    The return value from C API calls.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(_LIB\u001b[39m.\u001b[39mLGBM_GetLastError()\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mLightGBMError\u001b[0m: GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=CFG.n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0d689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "best_value = study.best_value\n",
    "print(\"best_param: {}\\n\\nbest_value: {}\".format(best_params, best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0b9528",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# best_paramsã«ã¯å›ºå®šå€¤ãŒä¿å­˜ã•ã‚Œãªã„ã®ã§æ”¹ã‚ã¦è¨­å®š\n",
    "best_params[\"boosting_type\"] = CFG.boosting_type\n",
    "best_params[\"device_type\"] = CFG.device_type\n",
    "best_params[\"seed\"] = CFG.seed\n",
    "best_params[\"n_jobs\"] = -1\n",
    "best_params[\"is_unbalance\"] = True\n",
    "best_params[\"objective\"] = \"binary\"\n",
    "best_params[\"learning_rate\"] = 0.005\n",
    "best_params[\"metric\"] = \"binary_logloss\"\n",
    "best_params[\"verbosity\"] = -1\n",
    "\n",
    "# è¨­å®šã—ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŸºã«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ\n",
    "model = lgb.LGBMClassifier(**best_params)\n",
    "# å­¦ç¿’\n",
    "model.fit(train_df[features], train_df[CFG.target_col])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# äºˆæ¸¬\n",
    "prediction = model.predict_proba(test_df.drop([\"Id\", \"Class\"], axis=1))\n",
    "# æå‡ºç”¨ã«å€¤ã‚’å¤‰æ›\n",
    "prediction = max(min(prediction, 1-10**(-15)), 10**(-15))\n",
    "submission = pd.DataFrame(columns = submission_df.columns)\n",
    "submission['Id'] = test_df['Id']\n",
    "submission[['class_0','class_1']] = prediction\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "submission\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
