{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2ebfbb",
   "metadata": {},
   "source": [
    "# preprocessed_xgboost\n",
    "* 何もなし  \n",
    "CV: 0.28495961062567476\n",
    "* 欠損値補完  \n",
    "CV: 0.2937347187995721\n",
    "* DV, AR削除  \n",
    "CV: 0.29048520579559434\n",
    "* Robust Scaler  \n",
    "CV: 0.30632285148599936\n",
    "* 欠損値補完+特徴量抽出+Robust Scaler  \n",
    "CV: 0.30632285148599936  \n",
    "LB: 0.24\n",
    "* 欠損値補完+特徴量抽出+Robust Scaler+外れ値除去  \n",
    "CV: 0.2904466303611915  \n",
    "LB: 0.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c565dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE # SMOTE\n",
    "from sklearn.impute import KNNImputer # kNN Imputation\n",
    "from sklearn.feature_selection import SelectKBest, f_classif# Feature Selection\n",
    "# Data Encoder and Scaler\n",
    "import category_encoders as encoders\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c555c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    '''設定値を格納'''\n",
    "    num_boost_round = 926\n",
    "    early_stopping_rounds = 98\n",
    "    n_folds = 5 # 公差検証の分割数\n",
    "    n_trials = 100 # ハイパーパラメータチューニングの試行回数\n",
    "    seed = 1234\n",
    "    # xgboost設定値\n",
    "    xgb_params = {\n",
    "        'objective': 'binary:logistic',# 学習タスク\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'eval_metric': 'rmse',\n",
    "        'random_state': seed,\n",
    "        'learning_rate': 0.01,\n",
    "        # 探索したパラメータ\n",
    "        'max_depth': 20,\n",
    "        'colsample_bytree': 0.6597946356014663,\n",
    "        'subsample': 0.7,\n",
    "        'gamma': 1.884172979513442,\n",
    "        'lambda': 0.006050024963423825,\n",
    "        'min_child_weight': 9,\n",
    "    }\n",
    "    \n",
    "class Preprocessing:\n",
    "    '''前処理を行うクラス'''\n",
    "    def __init__(self, train_df, test_df):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.numerical_columns = train_df.drop(['Id', 'EJ', 'Class'], axis=1).columns\n",
    "        self.features = pd.DataFrame(index=self.numerical_columns, columns=[\"F_value\", \"p_value\"])\n",
    "        \n",
    "    def knn_imputer(self):\n",
    "        # インスタンス生成\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        \n",
    "        # ローカル変数に値を格納\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "        \n",
    "        # 訓練データに欠損値代入\n",
    "        train_df_imputed = pd.DataFrame(imputer.fit_transform(temp_train_df[self.numerical_columns]), columns=self.numerical_columns)\n",
    "        \n",
    "        # テストデータに欠損値代入\n",
    "        test_df_imputed = pd.DataFrame(imputer.transform(temp_test_df[self.numerical_columns]), columns=self.numerical_columns)\n",
    "\n",
    "        # 元の訓練データも欠損値を補完したデータに置き換える\n",
    "        temp_train_df = temp_train_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_train_df = pd.concat([temp_train_df, train_df_imputed], axis=1)\n",
    "\n",
    "        # テストデータを欠損値を代入したデータに置き換える\n",
    "        temp_test_df = temp_test_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_test_df = pd.concat([temp_test_df, test_df_imputed], axis=1)\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "    \n",
    "    def clip_outliers(self):\n",
    "        # ローカル変数に値を格納\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "\n",
    "        first_quartiles = temp_train_df[self.numerical_columns].quantile(0.25) # 第１四分位数\n",
    "        third_quartiles = temp_train_df[self.numerical_columns].quantile(0.75) # 第３四分位数\n",
    "        iqr = third_quartiles - first_quartiles # 四分位範囲\n",
    "\n",
    "        lower_bound = first_quartiles - (iqr * 1.5) #外れ値の下限\n",
    "        upper_bound = third_quartiles + (iqr * 1.5) #外れ値の上限\n",
    "\n",
    "        # 訓練データとテストデータの両方に対して処理を行う\n",
    "        for df in [temp_train_df, temp_test_df]:\n",
    "            df[self.numerical_columns] = df[self.numerical_columns].clip(lower_bound, upper_bound, axis=1)\n",
    "\n",
    "        return temp_train_df, temp_test_df\n",
    "        \n",
    "    def robust_scaler(self):\n",
    "        # インスタンス生成\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        # ローカル変数に値を格納\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "\n",
    "        '''訓練データのスケーリング'''\n",
    "        # インデックスを抽出\n",
    "        index = temp_train_df.index\n",
    "        # スケーリング\n",
    "        scaler_train = scaler.fit_transform(temp_train_df[self.numerical_columns])\n",
    "        scaled_train_df = pd.DataFrame(scaler_train, columns=self.numerical_columns)\n",
    "        # インデックスを振りなおす\n",
    "        scaled_train_df.index = index\n",
    "\n",
    "        '''テストデータのスケーリング'''\n",
    "        # インデックスを抽出\n",
    "        index = temp_test_df.index\n",
    "        # スケーリング\n",
    "        scaler_test = scaler.fit_transform(temp_test_df[self.numerical_columns])\n",
    "        scaled_test_df = pd.DataFrame(scaler_test, columns=self.numerical_columns)\n",
    "        # インデックスを振りなおす\n",
    "        scaled_test_df.index = index\n",
    "        \n",
    "        # 元の訓練データも欠損値を補完したデータに置き換える\n",
    "        temp_train_df = temp_train_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_train_df = pd.concat([temp_train_df, scaled_train_df], axis=1)\n",
    "\n",
    "        # テストデータを欠損値を代入したデータに置き換える\n",
    "        temp_test_df = temp_test_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_test_df = pd.concat([temp_test_df, scaled_test_df], axis=1)\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "    \n",
    "    def select_k_best(self, pvalue_upper_limit = 0.1, fscore_lower_limit = 5):\n",
    "        # ローカル変数に値を格納\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "        \n",
    "        # 訓練データを説明変数と目的変数に分割\n",
    "        X_train = temp_train_df.drop(['Id', 'EJ', 'Class'], axis=1)\n",
    "        y_train = temp_train_df['Class']\n",
    "        # y_train.columns = ['Class']\n",
    "        '''F値とp値を計算'''\n",
    "        # インスタンス生成\n",
    "        #     回帰: f_regression, mutual_info_regression\n",
    "        #     分類: chi2, f_classif(分散分析のF値), mutual_info_classif\n",
    "        # この時点ではkをもとの訓練データと同じにする\n",
    "        fs = SelectKBest(score_func=f_classif, k=len(X_train.columns))\n",
    "        # 特徴量選択\n",
    "        X_selected = fs.fit_transform(X_train, y_train.values)\n",
    "\n",
    "        '''選択したF値とp値と設定した閾値を用いて特徴量を選択'''\n",
    "        new_features = [] # 選択された特徴量を格納\n",
    "        drop_features = [] # 使わない特徴量を格納\n",
    "\n",
    "        # F値が大きく、p値の小さい特徴量を選択\n",
    "        for i in range(len(X_train.columns)):\n",
    "            # F値とp値を格納\n",
    "            self.features.loc[X_train.columns[i], \"F_value\"] = fs.scores_[i]\n",
    "            self.features.loc[X_train.columns[i], \"p_value\"] = fs.pvalues_[i]\n",
    "            \n",
    "            if fs.pvalues_[i] <= pvalue_upper_limit and fs.scores_[i] >= fscore_lower_limit:\n",
    "                new_features.append(X_train.columns[i])\n",
    "            else:\n",
    "                drop_features.append(X_train.columns[i])\n",
    "\n",
    "        # 訓練データから選択した特徴量を抽出        \n",
    "        X_selected_final = pd.DataFrame(X_selected)\n",
    "        X_selected_final.columns = X_train.columns\n",
    "        X_selected_final = X_selected_final[new_features]\n",
    "        # print('=' * 30)\n",
    "        # print('After the SelectKBest = {}'.format(X_selected_final.shape))\n",
    "        # print('Drop-out Features = {}'.format(len(drop_features)))\n",
    "\n",
    "        # 元のデータに反映\n",
    "        # X_train = X_train.drop(drop_features, axis=1)\n",
    "        temp_train_df = temp_train_df.drop(drop_features, axis=1)\n",
    "        temp_test_df = temp_test_df.drop(drop_features, axis=1)\n",
    "        \n",
    "        self.features = self.features.loc[new_features, :] # 選択された特徴量だけをfeaturesに保存\n",
    "        self.features = self.features.sort_values(\"F_value\", ascending=False)# F値が大きい順にソート\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "        \n",
    "def preprocessing_pipeline(train_df, test_df):\n",
    "    # クラスのインスタンスを生成\n",
    "    preprocessor = Preprocessing(train_df, test_df)\n",
    "    \n",
    "    # 各メソッドを順に実行\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.knn_imputer() # 欠損値代入\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.clip_outliers() # 外れ値除去\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.robust_scaler() # スケーリング\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.select_k_best(pvalue_upper_limit = 0.1, fscore_lower_limit = 5) # 特徴量選択\n",
    "    \n",
    "    print('selected features: \\n{}'.format(preprocessor.features))\n",
    "\n",
    "    # 最終的に処理されたデータフレームを返す\n",
    "    return preprocessor.train_df, preprocessor.test_df\n",
    "\n",
    "# 評価基準\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability 𝑝 is replaced with max(min(𝑝,1−10−15),10−15)\n",
    "    y_pred = np.maximum(np.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1-y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "# Classの０，１の割合をそれぞれ計算\n",
    "def calc_log_loss_weight(y_true):\n",
    "    nc = np.bincount(y_true)\n",
    "    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n",
    "    return w0, w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b8ce8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected features: \n",
      "        F_value   p_value\n",
      "DU   130.715488       0.0\n",
      "FL    83.888079       0.0\n",
      "AB    60.224852       0.0\n",
      "AF    53.517631       0.0\n",
      "CR    50.408764       0.0\n",
      "BQ    49.315472       0.0\n",
      "DI     45.89467       0.0\n",
      "EH    38.219507       0.0\n",
      "FD     36.58523       0.0\n",
      "BC    34.055825       0.0\n",
      "DA    31.591489       0.0\n",
      "DH    29.076517       0.0\n",
      "FE      26.7766       0.0\n",
      "BN    25.466834  0.000001\n",
      "CD    21.570755  0.000004\n",
      "FR    20.684355  0.000007\n",
      "EE    17.124726   0.00004\n",
      "BP    17.013367  0.000042\n",
      "GF    16.174458  0.000065\n",
      "EB    16.099116  0.000068\n",
      "DL     15.07087  0.000115\n",
      "DE    14.020601  0.000198\n",
      "CC    13.439736  0.000268\n",
      "AM    12.424851  0.000455\n",
      "FI    10.052604  0.001597\n",
      "GL     8.809108  0.003114\n",
      "CU      5.38575  0.020628\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "# BASE_DIR = '/kaggle/input/icr-identify-age-related-conditions/'\n",
    "BASE_DIR = '../../data'\n",
    "train_df = pd.read_csv(f'{BASE_DIR}/train.csv')\n",
    "greeks_df = pd.read_csv(f'{BASE_DIR}/greeks.csv')\n",
    "test_df = pd.read_csv(f'{BASE_DIR}/test.csv')\n",
    "submission_df = pd.read_csv(f'{BASE_DIR}/sample_submission.csv')\n",
    "\n",
    "# 前処理\n",
    "train_df, test_df = preprocessing_pipeline(train_df, test_df)\n",
    "\n",
    "# 訓練データを説明変数と目的変数に分割\n",
    "X_train = train_df.drop(['Id', 'EJ', 'Class'], axis=1)\n",
    "y_train = train_df['Class']\n",
    "y_train.columns = ['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f5345e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n",
      "our out of folds CV score is 0.2904466303611915\n"
     ]
    }
   ],
   "source": [
    "# 各分割ごとのテストデータに対する予測値を格納\n",
    "preds = np.zeros(len(test_df.drop([\"Id\", 'EJ'], axis=1)))\n",
    "# 各分割ごとのバリデーションスコアを格納\n",
    "scores = 0\n",
    "\n",
    "# K-分割交差検証(層化抽出法)\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # 進行状況\n",
    "    print('fold: {}'.format(fold+1))\n",
    "    # 訓練データを分割\n",
    "    X_train_fold = X_train.iloc[train_index]\n",
    "    y_train_fold = y_train.iloc[train_index]\n",
    "    X_valid_fold = X_train.iloc[valid_index]\n",
    "    y_valid_fold = y_train.iloc[valid_index]\n",
    "    \n",
    "    # インスタンス生成\n",
    "    # smote = SMOTE(sampling_strategy={0: 408, 1: 408})\n",
    "    # オーバーサンプリング\n",
    "    # X_train_fold, y_train_fold = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "\n",
    "    # 訓練データの重みを計算\n",
    "    train_w0, train_w1 = calc_log_loss_weight(y_train_fold)\n",
    "    # 検証データの重みを計算\n",
    "    valid_w0, valid_w1 = calc_log_loss_weight(y_valid_fold)\n",
    "    # 訓練データをxgb用に変換\n",
    "    xgb_train = xgb.DMatrix(data=X_train_fold, label=y_train_fold, weight=y_train_fold.map({0: train_w0, 1: train_w1}))\n",
    "    # 検証データをxgb用に変換\n",
    "    xgb_valid = xgb.DMatrix(data=X_valid_fold, label=y_valid_fold, weight=y_valid_fold.map({0: valid_w0, 1: valid_w1}))\n",
    "\n",
    "    # モデルのインスタンス生成\n",
    "    model = xgb.train(\n",
    "        CFG.xgb_params, \n",
    "        dtrain = xgb_train, \n",
    "        num_boost_round = CFG.num_boost_round,\n",
    "        evals = [(xgb_train, 'train'), (xgb_valid, 'eval')], \n",
    "        early_stopping_rounds = CFG.early_stopping_rounds,\n",
    "        verbose_eval = False, # 整数に設定すると、n回ごとのブースティングステージで評価メトリクスを表示\n",
    "    )\n",
    "    # 検証\n",
    "    valid_preds = model.predict(xgb.DMatrix(X_valid_fold), iteration_range=(0, model.best_ntree_limit))\n",
    "    # 予測\n",
    "    pred = model.predict(xgb.DMatrix(test_df.drop(['Id', 'EJ'], axis=1)), iteration_range=(0, model.best_ntree_limit))\n",
    "    \n",
    "    # 予測値をラベルに変換\n",
    "    # pred_labels = np.rint(preds)\n",
    "    # 評価\n",
    "    # val_score = balanced_log_loss(y_valid, pred_labels)\n",
    "    val_score = balanced_log_loss(y_valid_fold, valid_preds)\n",
    "\n",
    "    # 予測を保存\n",
    "    preds += pred\n",
    "    # スコアを保存\n",
    "    scores += val_score\n",
    "    \n",
    "# クロスバリデーションの平均値を計算\n",
    "test_pred = preds / CFG.n_folds\n",
    "cv_score = scores /  CFG.n_folds\n",
    "print(f'our out of folds CV score is {cv_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a054954d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.993401</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.993401</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.993401</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.993401</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.993401</td>\n",
       "      <td>0.006599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id   class_0   class_1\n",
       "0  00eed32682bb  0.993401  0.006599\n",
       "1  010ebe33f668  0.993401  0.006599\n",
       "2  02fa521e1838  0.993401  0.006599\n",
       "3  040e15f562a2  0.993401  0.006599\n",
       "4  046e85c7cc7f  0.993401  0.006599"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提出用に値を変換\n",
    "submission = pd.DataFrame(columns = submission_df.columns)\n",
    "submission['Id'] = test_df['Id']\n",
    "submission['class_0'] = 1 - test_pred\n",
    "submission['class_1'] = test_pred\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
