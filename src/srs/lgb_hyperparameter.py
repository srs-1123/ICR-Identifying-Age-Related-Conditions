import numpy as np
import pandas as pd
from sklearn.metrics import log_loss
from sklearn.model_selection import StratifiedKFold
import optuna
import warnings
import xgboost as xgb
from imblearn.over_sampling import SMOTE # SMOTE
from sklearn.impute import KNNImputer # kNN Imputation
from sklearn.feature_selection import SelectKBest, f_classif# Feature Selection
# Data Encoder and Scaler
import category_encoders as encoders
from sklearn.preprocessing import LabelEncoder, RobustScaler
import pickle
warnings.simplefilter('ignore')

class CFG:
    '''è¨­å®šå€¤ã‚’æ ¼ç´'''
    num_boost_round = 926
    early_stopping_rounds = 98
    n_folds = 5 # å…¬å·®æ¤œè¨¼ã®åˆ†å‰²æ•°
    seed = 1234
    learning_rate = 0.01
    
class Preprocessing:
    '''å‰å‡¦ç†ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹'''
    def __init__(self, train_df, test_df):
        self.train_df = train_df
        self.test_df = test_df
        self.numerical_columns = train_df.drop(['Id', 'EJ', 'Class'], axis=1).columns
        self.features = pd.DataFrame(index=self.numerical_columns, columns=["F_value", "p_value"])
        
    def knn_imputer(self):
        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
        imputer = KNNImputer(n_neighbors=5)
        
        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´
        temp_train_df = self.train_df
        temp_test_df = self.test_df
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ä»£å…¥
        train_df_imputed = pd.DataFrame(imputer.fit_transform(temp_train_df[self.numerical_columns]), columns=self.numerical_columns)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ä»£å…¥
        test_df_imputed = pd.DataFrame(imputer.transform(temp_test_df[self.numerical_columns]), columns=self.numerical_columns)

        # å…ƒã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚‚æ¬ æå€¤ã‚’è£œå®Œã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹
        temp_train_df = temp_train_df.drop(self.numerical_columns, axis=1)
        temp_train_df = pd.concat([temp_train_df, train_df_imputed], axis=1)

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ¬ æå€¤ã‚’ä»£å…¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹
        temp_test_df = temp_test_df.drop(self.numerical_columns, axis=1)
        temp_test_df = pd.concat([temp_test_df, test_df_imputed], axis=1)
        
        return temp_train_df, temp_test_df
    
    def clip_outliers(self):
        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´
        temp_train_df = self.train_df
        temp_test_df = self.test_df

        first_quartiles = temp_train_df[self.numerical_columns].quantile(0.25) # ç¬¬ï¼‘å››åˆ†ä½æ•°
        third_quartiles = temp_train_df[self.numerical_columns].quantile(0.75) # ç¬¬ï¼“å››åˆ†ä½æ•°
        iqr = third_quartiles - first_quartiles # å››åˆ†ä½ç¯„å›²

        lower_bound = first_quartiles - (iqr * 1.5) #å¤–ã‚Œå€¤ã®ä¸‹é™
        upper_bound = third_quartiles + (iqr * 1.5) #å¤–ã‚Œå€¤ã®ä¸Šé™

        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä¸¡æ–¹ã«å¯¾ã—ã¦å‡¦ç†ã‚’è¡Œã†
        for df in [temp_train_df, temp_test_df]:
            df[self.numerical_columns] = df[self.numerical_columns].clip(lower_bound, upper_bound, axis=1)

        return temp_train_df, temp_test_df
        
    def robust_scaler(self):
        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
        scaler = RobustScaler()
        
        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´
        temp_train_df = self.train_df
        temp_test_df = self.test_df

        '''è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°'''
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŠ½å‡º
        index = temp_train_df.index
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler_train = scaler.fit_transform(temp_train_df[self.numerical_columns])
        scaled_train_df = pd.DataFrame(scaler_train, columns=self.numerical_columns)
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒ¯ã‚ŠãªãŠã™
        scaled_train_df.index = index

        '''ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°'''
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŠ½å‡º
        index = temp_test_df.index
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler_test = scaler.fit_transform(temp_test_df[self.numerical_columns])
        scaled_test_df = pd.DataFrame(scaler_test, columns=self.numerical_columns)
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒ¯ã‚ŠãªãŠã™
        scaled_test_df.index = index
        
        # å…ƒã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚‚æ¬ æå€¤ã‚’è£œå®Œã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹
        temp_train_df = temp_train_df.drop(self.numerical_columns, axis=1)
        temp_train_df = pd.concat([temp_train_df, scaled_train_df], axis=1)

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ¬ æå€¤ã‚’ä»£å…¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹
        temp_test_df = temp_test_df.drop(self.numerical_columns, axis=1)
        temp_test_df = pd.concat([temp_test_df, scaled_test_df], axis=1)
        
        return temp_train_df, temp_test_df
    
    def select_k_best(self, pvalue_upper_limit = 0.1, fscore_lower_limit = 5):
        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´
        temp_train_df = self.train_df
        temp_test_df = self.test_df
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°ã«åˆ†å‰²
        X_train = temp_train_df.drop(['Id', 'EJ', 'Class'], axis=1)
        y_train = temp_train_df['Class']
        # y_train.columns = ['Class']
        '''Få€¤ã¨på€¤ã‚’è¨ˆç®—'''
        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
        #     å›å¸°: f_regression, mutual_info_regression
        #     åˆ†é¡: chi2, f_classif(åˆ†æ•£åˆ†æã®Få€¤), mutual_info_classif
        # ã“ã®æ™‚ç‚¹ã§ã¯kã‚’ã‚‚ã¨ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜ã«ã™ã‚‹
        fs = SelectKBest(score_func=f_classif, k=len(X_train.columns))
        # ç‰¹å¾´é‡é¸æŠ
        X_selected = fs.fit_transform(X_train, y_train.values)

        '''é¸æŠã—ãŸFå€¤ã¨på€¤ã¨è¨­å®šã—ãŸé–¾å€¤ã‚’ç”¨ã„ã¦ç‰¹å¾´é‡ã‚’é¸æŠ'''
        new_features = [] # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã‚’æ ¼ç´
        drop_features = [] # ä½¿ã‚ãªã„ç‰¹å¾´é‡ã‚’æ ¼ç´

        # Få€¤ãŒå¤§ããã€på€¤ã®å°ã•ã„ç‰¹å¾´é‡ã‚’é¸æŠ
        for i in range(len(X_train.columns)):
            # Få€¤ã¨på€¤ã‚’æ ¼ç´
            self.features.loc[X_train.columns[i], "F_value"] = fs.scores_[i]
            self.features.loc[X_train.columns[i], "p_value"] = fs.pvalues_[i]
            
            if fs.pvalues_[i] <= pvalue_upper_limit and fs.scores_[i] >= fscore_lower_limit:
                new_features.append(X_train.columns[i])
            else:
                drop_features.append(X_train.columns[i])

        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é¸æŠã—ãŸç‰¹å¾´é‡ã‚’æŠ½å‡º        
        X_selected_final = pd.DataFrame(X_selected)
        X_selected_final.columns = X_train.columns
        X_selected_final = X_selected_final[new_features]
        # print('=' * 30)
        # print('After the SelectKBest = {}'.format(X_selected_final.shape))
        # print('Drop-out Features = {}'.format(len(drop_features)))

        # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã«åæ˜ 
        # X_train = X_train.drop(drop_features, axis=1)
        temp_train_df = temp_train_df.drop(drop_features, axis=1)
        temp_test_df = temp_test_df.drop(drop_features, axis=1)
        
        self.features = self.features.loc[new_features, :] # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã ã‘ã‚’featuresã«ä¿å­˜
        self.features = self.features.sort_values("F_value", ascending=False)# Få€¤ãŒå¤§ãã„é †ã«ã‚½ãƒ¼ãƒˆ
        
        return temp_train_df, temp_test_df
        
def preprocessing_pipeline(train_df, test_df):
    # ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆ
    preprocessor = Preprocessing(train_df, test_df)
    
    # å„ãƒ¡ã‚½ãƒƒãƒ‰ã‚’é †ã«å®Ÿè¡Œ
    preprocessor.train_df, preprocessor.test_df = preprocessor.knn_imputer() # æ¬ æå€¤ä»£å…¥
    # preprocessor.train_df, preprocessor.test_df = preprocessor.clip_outliers() # å¤–ã‚Œå€¤é™¤å»
    # preprocessor.train_df, preprocessor.test_df = preprocessor.robust_scaler() # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    preprocessor.train_df, preprocessor.test_df = preprocessor.select_k_best(pvalue_upper_limit = 0.1, fscore_lower_limit = 5) # ç‰¹å¾´é‡é¸æŠ
    
    # print('selected features: \n{}'.format(preprocessor.features))

    # æœ€çµ‚çš„ã«å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™
    return preprocessor.train_df, preprocessor.test_df

# è©•ä¾¡åŸºæº–
def balanced_log_loss(y_true, y_pred):
    N = len(y_true)

    # Nc is the number of observations
    N_1 = np.sum(y_true == 1, axis=0)
    N_0 = np.sum(y_true == 0, axis=0)

    # In order to avoid the extremes of the log function, each predicted probability ğ‘ is replaced with max(min(ğ‘,1âˆ’10âˆ’15),10âˆ’15)
    y_pred = np.maximum(np.minimum(y_pred, 1 - 1e-15), 1e-15)

    # balanced logarithmic loss
    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1-y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))

    return loss_numerator / 2

# Classã®ï¼ï¼Œï¼‘ã®å‰²åˆã‚’ãã‚Œãã‚Œè¨ˆç®—
def calc_log_loss_weight(y_true):
    nc = np.bincount(y_true)
    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])
    return w0, w1

def objective(trial):
    # xgboostè¨­å®šå€¤
    xgb_params = {
        'objective': 'binary:logistic',# å­¦ç¿’ã‚¿ã‚¹ã‚¯
        'tree_method': 'gpu_hist',
        'eval_metric': 'rmse',
        'random_state': CFG.seed,
        'learning_rate': CFG.learning_rate,
        # æ¢ç´¢ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        'max_depth': trial.suggest_int('max_depth', 1, 50),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
        'subsample': trial.suggest_categorical('subsample', [0.4, 0.5, 0.6, 0.7, 0.8, 1.0]),
        'gamma': trial.suggest_uniform('gamma', 0, 2),
        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),
    }
    
    scores = []
    # K-åˆ†å‰²äº¤å·®æ¤œè¨¼(å±¤åŒ–æŠ½å‡ºæ³•)
    kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)
    
    for fold, (train_index, valid_index) in enumerate(kfold.split(X_train, y_train)):
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
        X_train_fold = X_train.iloc[train_index]
        y_train_fold = y_train.iloc[train_index]
        X_valid_fold = X_train.iloc[valid_index]
        y_valid_fold = y_train.iloc[valid_index]
    
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ã‚’è¨ˆç®—
        train_w0, train_w1 = calc_log_loss_weight(y_train_fold)
        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ã‚’è¨ˆç®—
        valid_w0, valid_w1 = calc_log_loss_weight(y_valid_fold)
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’xgbç”¨ã«å¤‰æ›
        xgb_train = xgb.DMatrix(data=X_train_fold, label=y_train_fold, weight=y_train_fold.map({0: train_w0, 1: train_w1}))
        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’xgbç”¨ã«å¤‰æ›
        xgb_valid = xgb.DMatrix(data=X_valid_fold, label=y_valid_fold, weight=y_valid_fold.map({0: valid_w0, 1: valid_w1}))
        
        # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
        model = xgb.train(
            xgb_params, 
            dtrain = xgb_train, 
            num_boost_round = CFG.num_boost_round,
            evals = [(xgb_train, 'train'), (xgb_valid, 'eval')], 
            early_stopping_rounds = CFG.early_stopping_rounds,
            verbose_eval = False, # æ•´æ•°ã«è¨­å®šã™ã‚‹ã¨ã€nå›ã”ã¨ã®ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹ãƒ†ãƒ¼ã‚¸ã§è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º
        )
        # äºˆæ¸¬
        preds = model.predict(xgb.DMatrix(X_valid_fold), iteration_range=(0, model.best_ntree_limit))
        # äºˆæ¸¬å€¤ã‚’ãƒ©ãƒ™ãƒ«ã«å¤‰æ›
        # pred_labels = np.rint(preds)
        # è©•ä¾¡
        # val_score = balanced_log_loss(y_valid, pred_labels)
        val_score = balanced_log_loss(y_valid_fold, preds)
        
        scores.append(val_score)
    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®å¹³å‡å€¤ã‚’è¨ˆç®—
    mean_score = np.mean(scores)
    
    return mean_score

if __name__ == '__main__':
    n_trials = 100 # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®è©¦è¡Œå›æ•°
    BASE_DIR = 'data'
    train_df = pd.read_csv(f'{BASE_DIR}/train.csv')
    greeks_df = pd.read_csv(f'{BASE_DIR}/greeks.csv')
    test_df = pd.read_csv(f'{BASE_DIR}/test.csv')
    submission_df = pd.read_csv(f'{BASE_DIR}/sample_submission.csv')

    # å‰å‡¦ç†
    train_df, test_df = preprocessing_pipeline(train_df, test_df)

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°ã«åˆ†å‰²
    X_train = train_df.drop(['Id', 'EJ', 'Class'], axis=1)
    y_train = train_df['Class']
    y_train.columns = ['Class']

    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=n_trials)

    # çµæœã‚’pickleãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
    with open('src/srs/params/lgb_best_param.pkl', 'wb') as f:
        pickle.dump(study, f)