{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52304d70",
   "metadata": {},
   "source": [
    "# preprocessed_xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443f30b",
   "metadata": {},
   "source": [
    "## å®Ÿé¨“çµæœ\n",
    "* ä½•ã‚‚ãªã—  \n",
    "CV: 0.28495961062567476\n",
    "* æ¬ æå€¤è£œå®Œ  \n",
    "CV: 0.2937347187995721\n",
    "* DV, ARå‰Šé™¤  \n",
    "CV: 0.29048520579559434\n",
    "* Robust Scaler  \n",
    "CV: 0.30632285148599936\n",
    "* æ¬ æå€¤è£œå®Œ+ç‰¹å¾´é‡æŠ½å‡º+Robust Scaler  \n",
    "CV: 0.30632285148599936  \n",
    "LB: 0.24\n",
    "* æ¬ æå€¤è£œå®Œ+ç‰¹å¾´é‡æŠ½å‡º+Robust Scaler+å¤–ã‚Œå€¤é™¤å»  \n",
    "CV: 0.2904466303611915  \n",
    "LB: 0.26\n",
    "## 7/29å¤‰æ›´ç‚¹\n",
    "1. å¤–ã‚Œå€¤é™¤å»: å¤–ã‚Œå€¤ã®ä¸Šé™ä¸‹é™ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ¥ã€…ã«è¨ˆç®—\n",
    "2. train_integerizedã‚’ç‰¹å¾´é‡ã«ä½¿ç”¨\n",
    "* æ¬ æå€¤è£œå®Œ+ç‰¹å¾´é‡æŠ½å‡º+Robust Scaler+train_integerized+ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´\n",
    "CV: 0.2970240285484198\n",
    "* æ¬ æå€¤è£œå®Œ+ç‰¹å¾´é‡æŠ½å‡º+Robust Scaler+train_integerized+ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´+fevalã‚’balanced_log_lossã«å¤‰æ›´\n",
    "CV: 0.5590221126039339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c565dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "import warnings\n",
    "from imblearn.over_sampling import SMOTE # SMOTE\n",
    "from sklearn.impute import KNNImputer # kNN Imputation\n",
    "from sklearn.feature_selection import SelectKBest, f_classif# Feature Selection\n",
    "# Data Encoder and Scaler\n",
    "import category_encoders as encoders\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a894e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'local'\n",
    "# env = 'kaggle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55269571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "if env == 'local':\n",
    "    BASE_DIR = '../../data'\n",
    "elif env == 'kaggle':\n",
    "    BASE_DIR = '/kaggle/input/icr-identify-age-related-conditions/'\n",
    "else:\n",
    "    raise ValueError(\"Invalid environment. Set env as 'local' or 'kaggle'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c555c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    '''å‰å‡¦ç†ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹'''\n",
    "    def __init__(self, train_df, test_df):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.numerical_columns = train_df.drop(['Id', 'EJ', 'Class'], axis=1).columns\n",
    "        self.features = pd.DataFrame(index=self.numerical_columns, columns=[\"F_value\", \"p_value\"])\n",
    "        \n",
    "    def knn_imputer(self):\n",
    "        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        \n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "        \n",
    "        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ä»£å…¥\n",
    "        train_df_imputed = pd.DataFrame(imputer.fit_transform(temp_train_df[self.numerical_columns]), columns=self.numerical_columns)\n",
    "        \n",
    "        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«æ¬ æå€¤ä»£å…¥\n",
    "        test_df_imputed = pd.DataFrame(imputer.transform(temp_test_df[self.numerical_columns]), columns=self.numerical_columns)\n",
    "\n",
    "        # å…ƒã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚‚æ¬ æå€¤ã‚’è£œå®Œã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹\n",
    "        temp_train_df = temp_train_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_train_df = pd.concat([temp_train_df, train_df_imputed], axis=1)\n",
    "\n",
    "        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ¬ æå€¤ã‚’ä»£å…¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹\n",
    "        temp_test_df = temp_test_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_test_df = pd.concat([temp_test_df, test_df_imputed], axis=1)\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "    \n",
    "    def clip_outliers_ver1(self):\n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "\n",
    "        first_quartiles = temp_train_df[self.numerical_columns].quantile(0.25) # ç¬¬ï¼‘å››åˆ†ä½æ•°\n",
    "        third_quartiles = temp_train_df[self.numerical_columns].quantile(0.75) # ç¬¬ï¼“å››åˆ†ä½æ•°\n",
    "        iqr = third_quartiles - first_quartiles # å››åˆ†ä½ç¯„å›²\n",
    "\n",
    "        lower_bound = first_quartiles - (iqr * 1.5) #å¤–ã‚Œå€¤ã®ä¸‹é™\n",
    "        upper_bound = third_quartiles + (iqr * 1.5) #å¤–ã‚Œå€¤ã®ä¸Šé™\n",
    "\n",
    "        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä¸¡æ–¹ã«å¯¾ã—ã¦å‡¦ç†ã‚’è¡Œã†\n",
    "        for df in [temp_train_df, temp_test_df]:\n",
    "            df[self.numerical_columns] = df[self.numerical_columns].clip(lower_bound, upper_bound, axis=1)\n",
    "\n",
    "        return temp_train_df, temp_test_df\n",
    "    \n",
    "    def clip_outliers(self):\n",
    "        '''è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§åˆ¥ã€…ã«ä¸Šé™ã€ä¸‹é™ã‚’è¨ˆç®—ã™ã‚‹ã‚ˆã†å¤‰æ›´'''\n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´\n",
    "        clipped_df = []\n",
    "        \n",
    "        for df in [self.train_df, self.test_df]:\n",
    "            temp_df = df\n",
    "            first_quartiles = temp_df[self.numerical_columns].quantile(0.25) # ç¬¬ï¼‘å››åˆ†ä½æ•°\n",
    "            third_quartiles = temp_df[self.numerical_columns].quantile(0.75) # ç¬¬ï¼“å››åˆ†ä½æ•°\n",
    "            iqr = third_quartiles - first_quartiles # å››åˆ†ä½ç¯„å›²\n",
    "\n",
    "            lower_bound = first_quartiles - (iqr * 1.5) #å¤–ã‚Œå€¤ã®ä¸‹é™\n",
    "            upper_bound = third_quartiles + (iqr * 1.5) #å¤–ã‚Œå€¤ã®ä¸Šé™\n",
    "\n",
    "            temp_df[self.numerical_columns] = temp_df[self.numerical_columns].clip(lower_bound, upper_bound, axis=1)\n",
    "            clipped_df.append(temp_df)\n",
    "\n",
    "        return clipped_df[0], clipped_df[1]\n",
    "        \n",
    "    def robust_scaler(self):\n",
    "        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "\n",
    "        '''è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°'''\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŠ½å‡º\n",
    "        index = temp_train_df.index\n",
    "        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        scaler_train = scaler.fit_transform(temp_train_df[self.numerical_columns])\n",
    "        scaled_train_df = pd.DataFrame(scaler_train, columns=self.numerical_columns)\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒ¯ã‚ŠãªãŠã™\n",
    "        scaled_train_df.index = index\n",
    "\n",
    "        '''ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°'''\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŠ½å‡º\n",
    "        index = temp_test_df.index\n",
    "        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        scaler_test = scaler.fit_transform(temp_test_df[self.numerical_columns])\n",
    "        scaled_test_df = pd.DataFrame(scaler_test, columns=self.numerical_columns)\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒ¯ã‚ŠãªãŠã™\n",
    "        scaled_test_df.index = index\n",
    "        \n",
    "        # å…ƒã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚‚æ¬ æå€¤ã‚’è£œå®Œã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹\n",
    "        temp_train_df = temp_train_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_train_df = pd.concat([temp_train_df, scaled_train_df], axis=1)\n",
    "\n",
    "        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ¬ æå€¤ã‚’ä»£å…¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã‚‹\n",
    "        temp_test_df = temp_test_df.drop(self.numerical_columns, axis=1)\n",
    "        temp_test_df = pd.concat([temp_test_df, scaled_test_df], axis=1)\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "    \n",
    "    def select_k_best(self, pvalue_upper_limit = 0.1, fscore_lower_limit = 5):\n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã«å€¤ã‚’æ ¼ç´\n",
    "        temp_train_df = self.train_df\n",
    "        temp_test_df = self.test_df\n",
    "        \n",
    "        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°ã«åˆ†å‰²\n",
    "        X_train = temp_train_df.drop(['Id', 'EJ', 'Class'], axis=1)\n",
    "        y_train = temp_train_df['Class']\n",
    "        # y_train.columns = ['Class']\n",
    "        '''Få€¤ã¨på€¤ã‚’è¨ˆç®—'''\n",
    "        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "        #     å›å¸°: f_regression, mutual_info_regression\n",
    "        #     åˆ†é¡: chi2, f_classif(åˆ†æ•£åˆ†æã®Få€¤), mutual_info_classif\n",
    "        # ã“ã®æ™‚ç‚¹ã§ã¯kã‚’ã‚‚ã¨ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜ã«ã™ã‚‹\n",
    "        fs = SelectKBest(score_func=f_classif, k=len(X_train.columns))\n",
    "        # ç‰¹å¾´é‡é¸æŠ\n",
    "        X_selected = fs.fit_transform(X_train, y_train.values)\n",
    "\n",
    "        '''é¸æŠã—ãŸFå€¤ã¨på€¤ã¨è¨­å®šã—ãŸé–¾å€¤ã‚’ç”¨ã„ã¦ç‰¹å¾´é‡ã‚’é¸æŠ'''\n",
    "        new_features = [] # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã‚’æ ¼ç´\n",
    "        drop_features = [] # ä½¿ã‚ãªã„ç‰¹å¾´é‡ã‚’æ ¼ç´\n",
    "\n",
    "        # Få€¤ãŒå¤§ããã€på€¤ã®å°ã•ã„ç‰¹å¾´é‡ã‚’é¸æŠ\n",
    "        for i in range(len(X_train.columns)):\n",
    "            # Få€¤ã¨på€¤ã‚’æ ¼ç´\n",
    "            self.features.loc[X_train.columns[i], \"F_value\"] = fs.scores_[i]\n",
    "            self.features.loc[X_train.columns[i], \"p_value\"] = fs.pvalues_[i]\n",
    "            \n",
    "            if fs.pvalues_[i] <= pvalue_upper_limit and fs.scores_[i] >= fscore_lower_limit:\n",
    "                new_features.append(X_train.columns[i])\n",
    "            else:\n",
    "                drop_features.append(X_train.columns[i])\n",
    "\n",
    "        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é¸æŠã—ãŸç‰¹å¾´é‡ã‚’æŠ½å‡º        \n",
    "        X_selected_final = pd.DataFrame(X_selected)\n",
    "        X_selected_final.columns = X_train.columns\n",
    "        X_selected_final = X_selected_final[new_features]\n",
    "        # print('=' * 30)\n",
    "        # print('After the SelectKBest = {}'.format(X_selected_final.shape))\n",
    "        # print('Drop-out Features = {}'.format(len(drop_features)))\n",
    "\n",
    "        # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã«åæ˜ \n",
    "        # X_train = X_train.drop(drop_features, axis=1)\n",
    "        temp_train_df = temp_train_df.drop(drop_features, axis=1)\n",
    "        temp_test_df = temp_test_df.drop(drop_features, axis=1)\n",
    "        \n",
    "        self.features = self.features.loc[new_features, :] # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã ã‘ã‚’featuresã«ä¿å­˜\n",
    "        self.features = self.features.sort_values(\"F_value\", ascending=False)# Få€¤ãŒå¤§ãã„é †ã«ã‚½ãƒ¼ãƒˆ\n",
    "        \n",
    "        return temp_train_df, temp_test_df\n",
    "        \n",
    "def preprocessing_pipeline(train_df, test_df):\n",
    "    # ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆ\n",
    "    preprocessor = Preprocessing(train_df, test_df)\n",
    "    \n",
    "    # å„ãƒ¡ã‚½ãƒƒãƒ‰ã‚’é †ã«å®Ÿè¡Œ\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.knn_imputer() # æ¬ æå€¤ä»£å…¥\n",
    "    # preprocessor.train_df, preprocessor.test_df = preprocessor.clip_outliers() # å¤–ã‚Œå€¤é™¤å»\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.robust_scaler() # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "    preprocessor.train_df, preprocessor.test_df = preprocessor.select_k_best(pvalue_upper_limit = 0.1, fscore_lower_limit = 5) # ç‰¹å¾´é‡é¸æŠ\n",
    "    \n",
    "    # print('selected features: \\n{}'.format(preprocessor.features))\n",
    "\n",
    "    # æœ€çµ‚çš„ã«å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™\n",
    "    return preprocessor.train_df, preprocessor.test_df\n",
    "\n",
    "# è©•ä¾¡åŸºæº–\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability ğ‘ is replaced with max(min(ğ‘,1âˆ’10âˆ’15),10âˆ’15)\n",
    "    y_pred = np.maximum(np.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1-y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "def balanced_log_loss_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    loss = balanced_log_loss(y_true, y_pred)\n",
    "    return 'balanced_log_loss', loss\n",
    "\n",
    "# Classã®ï¼ï¼Œï¼‘ã®å‰²åˆã‚’ãã‚Œãã‚Œè¨ˆç®—\n",
    "def calc_log_loss_weight(y_true):\n",
    "    nc = np.bincount(y_true)\n",
    "    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n",
    "    return w0, w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba2e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_training(X_train, y_train, X_valid, y_valid):\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ã‚’è¨ˆç®—\n",
    "    train_w0, train_w1 = calc_log_loss_weight(y_train)\n",
    "    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ã‚’è¨ˆç®—\n",
    "    valid_w0, valid_w1 = calc_log_loss_weight(y_valid)\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’xgbç”¨ã«å¤‰æ›\n",
    "    xgb_train = xgb.DMatrix(data=X_train, label=y_train, weight=y_train.map({0: train_w0, 1: train_w1}))\n",
    "    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’xgbç”¨ã«å¤‰æ›\n",
    "    xgb_valid = xgb.DMatrix(data=X_valid, label=y_valid, weight=y_valid.map({0: valid_w0, 1: valid_w1}))\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "    model = xgb.train(\n",
    "        CFG.xgb_params, \n",
    "        dtrain = xgb_train, \n",
    "        num_boost_round = CFG.num_boost_round,\n",
    "        evals = [(xgb_train, 'train'), (xgb_valid, 'eval')], \n",
    "        # feval = balanced_log_loss_eval,\n",
    "        early_stopping_rounds = CFG.early_stopping_rounds,\n",
    "        verbose_eval = False, # æ•´æ•°ã«è¨­å®šã™ã‚‹ã¨ã€nå›ã”ã¨ã®ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¹ãƒ†ãƒ¼ã‚¸ã§è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º\n",
    "    )\n",
    "    # æ¤œè¨¼\n",
    "    valid_preds = model.predict(xgb.DMatrix(X_valid), iteration_range=(0, model.best_ntree_limit))\n",
    "    \n",
    "    return model, valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0015ff-0100-40af-8dc6-ccf78cd18c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle('params/xgb_best_param.pkl').best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38362eb8-3c17-4953-add7-f8d40914cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    '''è¨­å®šå€¤ã‚’æ ¼ç´'''\n",
    "    num_boost_round = 926\n",
    "    early_stopping_rounds = 98\n",
    "    n_folds = 5 # å…¬å·®æ¤œè¨¼ã®åˆ†å‰²æ•°\n",
    "    n_trials = 100 # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®è©¦è¡Œå›æ•°\n",
    "    seed = 1234\n",
    "    # xgboostè¨­å®šå€¤\n",
    "    xgb_params = {\n",
    "        'objective': 'binary:logistic',# å­¦ç¿’ã‚¿ã‚¹ã‚¯\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'random_state': seed,\n",
    "        'learning_rate': 0.01,\n",
    "        'eval_metric': 'rmse',\n",
    "        # æ¢ç´¢ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        'max_depth': 43,\n",
    "        'colsample_bytree': 0.9270015786178574,\n",
    "        'subsample': 1.0,\n",
    "        'gamma': 0.9008025641267255,\n",
    "        'lambda': 0.3150372040663734,\n",
    "        'min_child_weight': 7,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892075e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "if env == 'local':\n",
    "    BASE_DIR = '../../data'\n",
    "elif env == 'kaggle':\n",
    "    BASE_DIR = '/kaggle/input/icr-identify-age-related-conditions/'\n",
    "else:\n",
    "    raise ValueError(\"Invalid environment. Set env as 'local' or 'kaggle'.\")\n",
    "\n",
    "# train_df = pd.read_csv(f'{BASE_DIR}/train.csv')\n",
    "train_df = pd.read_csv(f'{BASE_DIR}/train_integerized.csv')\n",
    "greeks_df = pd.read_csv(f'{BASE_DIR}/greeks.csv')\n",
    "test_df = pd.read_csv(f'{BASE_DIR}/test.csv')\n",
    "submission_df = pd.read_csv(f'{BASE_DIR}/sample_submission.csv')\n",
    "\n",
    "# å‰å‡¦ç†\n",
    "train_df, test_df = preprocessing_pipeline(train_df, test_df)\n",
    "\n",
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°ã«åˆ†å‰²\n",
    "X_train = train_df.drop(['Id', 'EJ', 'Class'], axis=1)\n",
    "y_train = train_df['Class']\n",
    "y_train.columns = ['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f5345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "fold: 2\n",
      "fold: 3\n",
      "fold: 4\n",
      "fold: 5\n",
      "our out of folds CV score is 0.29338396162110936\n"
     ]
    }
   ],
   "source": [
    "# å„åˆ†å‰²ã”ã¨ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’æ ¼ç´\n",
    "scores = 0\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "models = []\n",
    "\n",
    "# K-åˆ†å‰²äº¤å·®æ¤œè¨¼(å±¤åŒ–æŠ½å‡ºæ³•)\n",
    "kfold = StratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
    "\n",
    "for fold, (train_index, valid_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # é€²è¡ŒçŠ¶æ³\n",
    "    print('fold: {}'.format(fold+1))\n",
    "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²\n",
    "    X_train_fold = X_train.iloc[train_index]\n",
    "    y_train_fold = y_train.iloc[train_index]\n",
    "    X_valid_fold = X_train.iloc[valid_index]\n",
    "    y_valid_fold = y_train.iloc[valid_index]\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã€äºˆæ¸¬ã‚’å‡ºåŠ›\n",
    "    model, valid_preds = xgb_training(X_train_fold, y_train_fold, X_valid_fold, y_valid_fold)\n",
    "\n",
    "    # è©•ä¾¡\n",
    "    val_score = balanced_log_loss(y_valid_fold, valid_preds)\n",
    "    # ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜\n",
    "    scores += val_score\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "    models.append(model)\n",
    "    \n",
    "# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®å¹³å‡å€¤ã‚’è¨ˆç®—\n",
    "cv_score = scores /  CFG.n_folds\n",
    "print(f'our out of folds CV score is {cv_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28a97355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# äºˆæ¸¬\n",
    "# å„åˆ†å‰²ã”ã¨ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äºˆæ¸¬å€¤ã‚’æ ¼ç´\n",
    "preds = np.zeros(len(test_df.drop([\"Id\", 'EJ'], axis=1)))\n",
    "for i in range(len(models)):\n",
    "    pred = models[i].predict(xgb.DMatrix(test_df.drop(['Id', 'EJ'], axis=1)), iteration_range=(0, models[i].best_iteration))\n",
    "    preds += pred\n",
    "test_pred = preds / CFG.n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a054954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå‡ºç”¨ã«å€¤ã‚’å¤‰æ›\n",
    "if env == 'kaggle':\n",
    "    submission = pd.DataFrame(columns = submission_df.columns)\n",
    "    submission['Id'] = test_df['Id']\n",
    "    submission['class_0'] = 1 - test_pred\n",
    "    submission['class_1'] = test_pred\n",
    "    submission.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
